---
year: 2020
date: 2020
authors:
  - "Li, Bohan"
  - "Zhou, Hao"
  - "He, Junxian"
  - "Wang, Mingxuan"
  - "Yang, Yiming"
  - "Li, Lei"
generated: true
key: 4GQBS92P
version: 2255
itemType: journalArticle
title: On the Sentence Embeddings from Pre-trained Language Models
publicationTitle: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)
pages: 9119-9130
language: en
DOI: 10.18653/v1/2020.emnlp-main.733
url: "https://www.aclweb.org/anthology/2020.emnlp-main.733"
accessDate: "2024-10-31T16:54:27Z"
libraryCatalog: Semantic Scholar
extra: "Conference Name: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) Place: Online Publisher: Association for Computational Linguistics"
collections:
  - ERQKEKFA
dateAdded: "2024-10-31T16:54:27Z"
dateModified: "2024-10-31T16:55:12Z"
super_collections:
  - ERQKEKFA
filename: Li et al. 2020 - On the Sentence Embeddings from Pre-trained Language Models
marker: "[ðŸ‡¿](zotero://select/library/items/4GQBS92P)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] On the Sentence Embeddings from Pre-trained Language Models

> [!example] File
> [Li et al. 2020 - On the Sentence Embeddings from Pre-trained Language Models](Li%20et%20al.%202020%20-%20On%20the%20Sentence%20Embeddings%20from%20Pre-trained%20Language%20Models.pdf)

> [!abstract] Abstract
> Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.

