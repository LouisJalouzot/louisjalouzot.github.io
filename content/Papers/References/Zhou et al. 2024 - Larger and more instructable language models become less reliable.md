---
year: 2024
month: 9
day: 25
date: 2024-09-25
authors:
  - "Zhou, Lexin"
  - "Schellaert, Wout"
  - "MartÃ­nez-Plumed, Fernando"
  - "Moros-Daval, Yael"
  - "Ferri, CÃ¨sar"
  - "HernÃ¡ndez-Orallo, JosÃ©"
generated: true
key: P5GZ5GQB
version: 2249
itemType: journalArticle
title: Larger and more instructable language models become less reliable
publicationTitle: Nature
journalAbbreviation: Nature
language: en
DOI: 10.1038/s41586-024-07930-y
ISSN: "0028-0836, 1476-4687"
url: "https://www.nature.com/articles/s41586-024-07930-y"
accessDate: "2024-10-01T15:41:59Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-10-01T15:41:59Z"
dateModified: "2024-10-01T15:42:48Z"
super_collections:
  - ERQKEKFA
filename: Zhou et al. 2024 - Larger and more instructable language models become less reliable
marker: "[ðŸ‡¿](zotero://select/library/items/P5GZ5GQB)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Larger and more instructable language models become less reliable

> [!example] File
> [Zhou et al. 2024 - Larger and more instructable language models become less reliable](Zhou%20et%20al.%202024%20-%20Larger%20and%20more%20instructable%20language%20models%20become%20less%20reliable.pdf)

> [!abstract] Abstract
> Abstract
>             
>               The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources
>               1
>               ) and bespoke shaping up (including post-filtering
>               2,3
>               , fine tuning or use of human feedback
>               4,5
>               ). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.

