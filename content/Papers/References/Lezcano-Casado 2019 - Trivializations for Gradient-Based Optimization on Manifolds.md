---
zoteroTags:
  - Computer Science - Machine Learning
  - Statistics - Machine Learning
year: 2019
month: 10
day: 24
date: 2019-10-24
authors:
  - "Lezcano-Casado, Mario"
generated: true
key: 9FUZGK2V
version: 2243
itemType: preprint
title: Trivializations for Gradient-Based Optimization on Manifolds
repository: arXiv
archiveID: "arXiv:1909.09501"
DOI: 10.48550/arXiv.1909.09501
url: "http://arxiv.org/abs/1909.09501"
accessDate: "2024-03-09T04:15:49Z"
libraryCatalog: arXiv.org
extra: "arXiv:1909.09501 [cs, stat]"
collections:
  - ERQKEKFA
dateAdded: "2024-03-09T04:15:49Z"
dateModified: "2024-03-09T04:16:04Z"
super_collections:
  - ERQKEKFA
filename: Lezcano-Casado 2019 - Trivializations for Gradient-Based Optimization on Manifolds
marker: "[ðŸ‡¿](zotero://select/library/items/9FUZGK2V)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Trivializations for Gradient-Based Optimization on Manifolds

> [!example] File
> [Lezcano-Casado 2019 - Trivializations for Gradient-Based Optimization on Manifolds](Lezcano-Casado%202019%20-%20Trivializations%20for%20Gradient-Based%20Optimization%20on%20Manifolds.pdf)

> [!abstract] Abstract
> We introduce a framework to study the transformation of problems with manifold constraints into unconstrained problems through parametrizations in terms of a Euclidean space. We call these parametrizations "trivializations". We prove conditions under which a trivialization is sound in the context of gradient-based optimization and we show how two large families of trivializations have overall favorable properties, but also suffer from a performance issue. We then introduce "dynamic trivializations", which solve this problem, and we show how these form a family of optimization methods that lie between trivializations and Riemannian gradient descent, and combine the benefits of both of them. We then show how to implement these two families of trivializations in practice for different matrix manifolds. To this end, we prove a formula for the gradient of the exponential of matrices, which can be of practical interest on its own. Finally, we show how dynamic trivializations improve the performance of existing methods on standard tasks designed to test long-term memory within neural networks.

