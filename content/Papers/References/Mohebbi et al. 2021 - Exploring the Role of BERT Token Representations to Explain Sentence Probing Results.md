---
year: 2021
date: 2021
authors:
  - "Mohebbi, Hosein"
  - "Modarressi, Ali"
  - "Pilehvar, Mohammad Taher"
generated: true
key: SQ2GCQCH
version: 2239
itemType: journalArticle
title: Exploring the Role of BERT Token Representations to Explain Sentence Probing Results
publicationTitle: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
pages: 792-806
language: en
DOI: 10.18653/v1/2021.emnlp-main.61
url: "https://aclanthology.org/2021.emnlp-main.61"
accessDate: "2024-04-03T16:29:33Z"
libraryCatalog: Semantic Scholar
extra: "Conference Name: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing Place: Online and Punta Cana, Dominican Republic Publisher: Association for Computational Linguistics"
collections:
  - ERQKEKFA
dateAdded: "2024-04-03T16:29:33Z"
dateModified: "2024-04-03T16:29:53Z"
super_collections:
  - ERQKEKFA
filename: Mohebbi et al. 2021 - Exploring the Role of BERT Token Representations to Explain Sentence Probing Results
marker: "[ðŸ‡¿](zotero://select/library/items/SQ2GCQCH)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Exploring the Role of BERT Token Representations to Explain Sentence Probing Results

> [!example] File
> [Mohebbi et al. 2021 - Exploring the Role of BERT Token Representations to Explain Sentence Probing Results](Mohebbi%20et%20al.%202021%20-%20Exploring%20the%20Role%20of%20BERT%20Token%20Representations%20to%20Explain%20Sentence%20Probing%20Results.pdf)

> [!abstract] Abstract
> Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model in encoding the corresponding linguistic property. Despite providing insights, these studies have left out the potential role of token representations. In this paper, we provide a more in-depth analysis on the representation space of BERT in search for distinct and meaningful subspaces that can explain the reasons behind these probing results. Based on a set of probing tasks and with the help of attribution methods we show that BERT tends to encode meaningful knowledge in specific token representations (which are often ignored in standard classification setups), allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces.

