---
zoteroTags:
  - Computer Science - Computation and Language
year: 2024
month: 2
day: 18
date: 2024-02-18
authors:
  - "Jalouzot, Louis"
  - "Sobczyk, Robin"
  - "Lhopitallier, Bastien"
  - "Salle, Jeanne"
  - "Lan, Nur"
  - "Chemla, Emmanuel"
  - "Lakretz, Yair"
generated: true
key: PA5J294R
version: 2239
itemType: preprint
title: Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations
repository: arXiv
archiveID: "arXiv:2402.11608"
DOI: 10.48550/arXiv.2402.11608
url: "http://arxiv.org/abs/2402.11608"
accessDate: "2024-03-28T16:15:51Z"
libraryCatalog: arXiv.org
extra: "arXiv:2402.11608 [cs]"
collections:
  - ERQKEKFA
dateAdded: "2024-03-28T16:15:51Z"
dateModified: "2024-03-28T16:15:51Z"
super_collections:
  - ERQKEKFA
filename: Jalouzot et al. 2024 - Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations
marker: "[ðŸ‡¿](zotero://select/library/items/PA5J294R)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations

> [!example] File
> [Jalouzot et al. 2024 - Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations](Jalouzot%20et%20al.%202024%20-%20Metric-Learning%20Encoding%20Models%20Identify%20Processing%20Profiles%20of%20Linguistic%20Features%20in%20BERT's%20Representations.pdf)

> [!abstract] Abstract
> We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univariate encoding methods, in being able to predict both local and distributed representations. Together, this demonstrates the utility of Metric-Learning Encoding Methods for studying how linguistic features are neurally encoded in language models and the advantage of MLEMs over traditional methods. MLEMs can be extended to other domains (e.g. vision) and to other neural systems, such as the human brain.

