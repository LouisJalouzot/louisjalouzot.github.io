---
year: 2024
month: 10
day: 28
date: 28 October 2024
authors:
  - "Nikankin, Yaniv"
  - "Reusch, Anja"
  - "Mueller, Aaron"
  - "Belinkov, Yonatan"
generated: true
key: JXZ7LLZ2
version: 2255
itemType: conferencePaper
title: "Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics"
shortTitle: Arithmetic Without Algorithms
url: "https://www.semanticscholar.org/paper/Arithmetic-Without-Algorithms%3A-Language-Models-Math-Nikankin-Reusch/48e669c2679b9acf7beb8abdb789167d61ceca49"
accessDate: "2024-11-04T09:39:07Z"
libraryCatalog: Semantic Scholar
deleted: 1
collections:
  - ERQKEKFA
dateAdded: "2024-11-04T09:39:07Z"
dateModified: "2024-11-04T09:39:17Z"
super_collections:
  - ERQKEKFA
filename: Nikankin et al. 2024 - Arithmetic Without Algorithms Language Models Solve Math With a Bag of Heuristics
marker: "[ðŸ‡¿](zotero://select/library/items/JXZ7LLZ2)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics

> [!example] File
> [Nikankin et al. 2024 - Arithmetic Without Algorithms Language Models Solve Math With a Bag of Heuristics](Nikankin%20et%20al.%202024%20-%20Arithmetic%20Without%20Algorithms%20Language%20Models%20Solve%20Math%20With%20a%20Bag%20of%20Heuristics.pdf)

> [!abstract] Abstract
> Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a"bag of heuristics".

