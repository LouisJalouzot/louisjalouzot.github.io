---
year: 2024
month: 5
day: 24
date: 24 May 2024
authors:
  - "Cheng, Emily"
  - "Doimo, Diego"
  - "Kervadec, Corentin"
  - "Macocco, Iuri"
  - "Yu, Jade"
  - "Laio, A."
  - "Baroni, Marco"
generated: true
key: 46N67X87
version: 2234
itemType: conferencePaper
title: Emergence of a High-Dimensional Abstraction Phase in Language Transformers
url: "https://www.semanticscholar.org/paper/Emergence-of-a-High-Dimensional-Abstraction-Phase-Cheng-Doimo/143224cc71d29805bf792a4576cdbfe1f60bd52b"
accessDate: "2024-05-31T20:33:41Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-05-31T20:33:41Z"
dateModified: "2024-05-31T20:33:46Z"
super_collections:
  - ERQKEKFA
filename: Cheng et al. 2024 - Emergence of a High-Dimensional Abstraction Phase in Language Transformers
marker: "[ðŸ‡¿](zotero://select/library/items/46N67X87)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Emergence of a High-Dimensional Abstraction Phase in Language Transformers

> [!example] File
> [Cheng et al. 2024 - Emergence of a High-Dimensional Abstraction Phase in Language Transformers](Cheng%20et%20al.%202024%20-%20Emergence%20of%20a%20High-Dimensional%20Abstraction%20Phase%20in%20Language%20Transformers.pdf)

> [!abstract] Abstract
> A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.

