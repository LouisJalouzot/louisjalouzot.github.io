---
zoteroTags:
  - Computer Science - Computation and Language
  - Computer Science - Machine Learning
year: 2024
month: 11
day: 19
date: 2024-12-19
authors:
  - "Qin, Tian"
  - "Saphra, Naomi"
  - "Alvarez-Melis, David"
generated: true
key: AJBYBKLT
version: 2723
itemType: preprint
paperTitle: "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization"
repository: arXiv
archiveID: "arXiv:2412.04619"
DOI: 10.48550/arXiv.2412.04619
url: "http://arxiv.org/abs/2412.04619"
accessDate: "2025-06-23T07:11:26Z"
shortTitle: Sometimes I am a Tree
libraryCatalog: arXiv.org
extra: "arXiv:2412.04619 [cs]"
dateAdded: "2025-06-23T07:11:26Z"
dateModified: "2025-06-23T07:11:26Z"
filename: Qin et al. 2024 - Sometimes I am a Tree Data Drives Unstable Hierarchical Generalization.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/AJBYBKLT)"
publish: true
type: reference
---
# Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization

[PDF file](/Papers/PDFs/Qin%20et%20al.%202024%20-%20Sometimes%20I%20am%20a%20Tree%20Data%20Drives%20Unstable%20Hierarchical%20Generalization.pdf)

> [!abstract] Abstract
> Language models (LMs), like other neural networks, often favor shortcut heuristics based on surface-level patterns. Although LMs behave like n-gram models early in training, they must eventually learn hierarchical syntactic representations to correctly apply grammatical rules out-of-distribution (OOD). In this work, we use case studies of English grammar to explore how complex, diverse training data drives models to generalize OOD. We construct a framework that unifies our understanding of random variation with training dynamics, rule selection with memorization, and data diversity with complexity. We show that these factors are nuanced, and that intermediate levels of diversity and complexity lead to inconsistent behavior across random seeds and to unstable training dynamics. Our findings emphasize the critical role of training data in shaping generalization patterns and illuminate how competing model strategies lead to inconsistent generalization outcomes across random seeds. Code is available at https://github.com/sunnytqin/concept_comp.git.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

