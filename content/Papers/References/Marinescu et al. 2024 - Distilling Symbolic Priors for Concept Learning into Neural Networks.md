---
zoteroTags:
  - Artificial Intelligence (cs.AI)
  - "FOS: Computer and information sciences"
  - Machine Learning (cs.LG)
year: 2024
date: 2024
authors:
  - "Marinescu, Ioana"
  - "McCoy, R. Thomas"
  - "Griffiths, Thomas L."
generated: true
key: 3B3RMB5C
version: 2254
itemType: journalArticle
title: Distilling Symbolic Priors for Concept Learning into Neural Networks
DOI: 10.48550/ARXIV.2402.07035
url: "https://arxiv.org/abs/2402.07035"
accessDate: "2024-10-29T10:26:51Z"
libraryCatalog: Semantic Scholar
rights: "arXiv.org perpetual, non-exclusive license"
extra: "Publisher: arXiv Version Number: 1"
collections:
  - ERQKEKFA
dateAdded: "2024-10-29T10:26:51Z"
dateModified: "2024-10-29T10:28:55Z"
super_collections:
  - ERQKEKFA
filename: Marinescu et al. 2024 - Distilling Symbolic Priors for Concept Learning into Neural Networks
marker: "[ðŸ‡¿](zotero://select/library/items/3B3RMB5C)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Distilling Symbolic Priors for Concept Learning into Neural Networks

> [!example] File
> [Marinescu et al. 2024 - Distilling Symbolic Priors for Concept Learning into Neural Networks](Marinescu%20et%20al.%202024%20-%20Distilling%20Symbolic%20Priors%20for%20Concept%20Learning%20into%20Neural%20Networks.pdf)

> [!abstract] Abstract
> Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks. By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-trained models are highly aligned with human performance.

