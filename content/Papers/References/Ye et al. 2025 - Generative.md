---
year: 2025
month: 2
day: 1
date: 2025-03-01
authors:
  - "Ye, Ziyi"
  - "Ai, Qingyao"
  - "Liu, Yiqun"
  - "De Rijke, Maarten"
  - "Zhang, Min"
  - "Lioma, Christina"
  - "Ruotsalo, Tuukka"
generated: true
key: TR8AZJGP
version: 2468
itemType: journalArticle
paperTitle: Generative language reconstruction from brain recordings
publicationTitle: Communications Biology
volume: 8
issue: 1
pages: 346
journalAbbreviation: Commun Biol
language: en
DOI: 10.1038/s42003-025-07731-7
ISSN: 2399-3642
url: "https://www.nature.com/articles/s42003-025-07731-7"
accessDate: "2025-04-10T11:51:29Z"
libraryCatalog: Semantic Scholar
dateAdded: "2025-04-10T11:51:29Z"
dateModified: "2025-04-10T11:51:29Z"
filename: Ye et al. 2025 - Generative language reconstruction from brain recordings.pdf
marker: "[🇿](zotero://select/library/items/TR8AZJGP)"
publish: true
type: reference
---
# Generative language reconstruction from brain recordings

[PDF file](/Papers/PDFs/Ye%20et%20al.%202025%20-%20Generative%20language%20reconstruction%20from%20brain%20recordings.pdf)

> [!abstract] Abstract
> Language reconstruction from non-invasive brain recordings has been a long-standing challenge. Existing research has addressed this challenge with a classification setup, where a set of language candidates are pre-constructed and then matched with the representation decoded from brain recordings. Here, we propose a method that addresses language reconstruction through auto-regressive generation, which directly uses the representation decoded from functional magnetic resonance imaging (fMRI) as the input for a large language model (LLM), mitigating the need for pre-constructed candidates. While an LLM can already generate high-quality content, our approach produces results more closely aligned with the visual or auditory language stimuli in response to which brain recordings are sampled, especially for content deemed “surprising” for the LLM. Furthermore, we show that the proposed approach can be used in an auto-regressive manner to reconstruct a 10 min-long language stimulus. Our method outperforms or is comparable to previous classification-based methods under different task settings, with the added benefit of estimating the likelihood of generating any semantic content. Our findings demonstrate the effectiveness of employing brain language interfaces in a generative setup and delineate a powerful and efficient means for mapping functional representations of language perception in the brain.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

