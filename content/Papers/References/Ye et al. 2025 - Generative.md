---
year: 2025
month: 2
day: 1
date: 2025-03-01
authors:
  - "Ye, Ziyi"
  - "Ai, Qingyao"
  - "Liu, Yiqun"
  - "De Rijke, Maarten"
  - "Zhang, Min"
  - "Lioma, Christina"
  - "Ruotsalo, Tuukka"
generated: true
key: TR8AZJGP
version: 2468
itemType: journalArticle
paperTitle: Generative language reconstruction from brain recordings
publicationTitle: Communications Biology
volume: 8
issue: 1
pages: 346
journalAbbreviation: Commun Biol
language: en
DOI: 10.1038/s42003-025-07731-7
ISSN: 2399-3642
url: "https://www.nature.com/articles/s42003-025-07731-7"
accessDate: "2025-04-10T11:51:29Z"
libraryCatalog: Semantic Scholar
dateAdded: "2025-04-10T11:51:29Z"
dateModified: "2025-04-10T11:51:29Z"
filename: Ye et al. 2025 - Generative language reconstruction from brain recordings.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/TR8AZJGP)"
publish: true
type: reference
---
# Generative language reconstruction from brain recordings

[PDF file](/Papers/PDFs/Ye%20et%20al.%202025%20-%20Generative%20language%20reconstruction%20from%20brain%20recordings.pdf)

> [!abstract] Abstract
> Language reconstruction from non-invasive brain recordings has been a long-standing challenge. Existing research has addressed this challenge with a classification setup, where a set of language candidates are pre-constructed and then matched with the representation decoded from brain recordings. Here, we propose a method that addresses language reconstruction through auto-regressive generation, which directly uses the representation decoded from functional magnetic resonance imaging (fMRI) as the input for a large language model (LLM), mitigating the need for pre-constructed candidates. While an LLM can already generate high-quality content, our approach produces results more closely aligned with the visual or auditory language stimuli in response to which brain recordings are sampled, especially for content deemed â€œsurprisingâ€ for the LLM. Furthermore, we show that the proposed approach can be used in an auto-regressive manner to reconstruct a 10â€‰min-long language stimulus. Our method outperforms or is comparable to previous classification-based methods under different task settings, with the added benefit of estimating the likelihood of generating any semantic content. Our findings demonstrate the effectiveness of employing brain language interfaces in a generative setup and delineate a powerful and efficient means for mapping functional representations of language perception in the brain.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

