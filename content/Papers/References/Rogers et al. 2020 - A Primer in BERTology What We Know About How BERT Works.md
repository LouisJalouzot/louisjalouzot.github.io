---
year: 2020
month: 12
date: 12/2020
authors:
  - "Rogers, Anna"
  - "Kovaleva, Olga"
  - "Rumshisky, Anna"
generated: true
key: P7DAMG6H
version: 2239
itemType: journalArticle
title: "A Primer in BERTology: What We Know About How BERT Works"
publicationTitle: Transactions of the Association for Computational Linguistics
volume: 8
pages: 842-866
journalAbbreviation: Transactions of the Association for Computational Linguistics
language: en
DOI: 10.1162/tacl_a_00349
ISSN: 2307-387X
shortTitle: A Primer in BERTology
url: "https://direct.mit.edu/tacl/article/96482"
accessDate: "2024-03-29T12:44:55Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-03-29T12:44:55Z"
dateModified: "2024-03-29T12:44:59Z"
super_collections:
  - ERQKEKFA
filename: Rogers et al. 2020 - A Primer in BERTology What We Know About How BERT Works
marker: "[ðŸ‡¿](zotero://select/library/items/P7DAMG6H)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] A Primer in BERTology: What We Know About How BERT Works

> [!example] File
> [Rogers et al. 2020 - A Primer in BERTology What We Know About How BERT Works](Rogers%20et%20al.%202020%20-%20A%20Primer%20in%20BERTology%20What%20We%20Know%20About%20How%20BERT%20Works.pdf)

> [!abstract] Abstract
> Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.

