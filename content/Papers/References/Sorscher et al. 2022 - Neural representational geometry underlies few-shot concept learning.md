---
year: 2022
month: 10
day: 25
date: 2022-10-25
authors:
  - "Sorscher, Ben"
  - "Ganguli, Surya"
  - "Sompolinsky, Haim"
generated: true
key: N2FX52M4
version: 2257
itemType: journalArticle
title: Neural representational geometry underlies few-shot concept learning
publicationTitle: Proceedings of the National Academy of Sciences
volume: 119
issue: 43
pages: e2200800119
journalAbbreviation: Proc. Natl. Acad. Sci. U.S.A.
language: en
DOI: 10.1073/pnas.2200800119
ISSN: "0027-8424, 1091-6490"
url: "https://pnas.org/doi/10.1073/pnas.2200800119"
accessDate: "2024-11-14T15:02:13Z"
libraryCatalog: Semantic Scholar
deleted: 1
collections:
  - ERQKEKFA
dateAdded: "2024-11-14T15:02:13Z"
dateModified: "2024-11-14T15:02:31Z"
super_collections:
  - ERQKEKFA
filename: Sorscher et al. 2022 - Neural representational geometry underlies few-shot concept learning
marker: "[ðŸ‡¿](zotero://select/library/items/N2FX52M4)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Neural representational geometry underlies few-shot concept learning

> [!example] File
> [Sorscher et al. 2022 - Neural representational geometry underlies few-shot concept learning](Sorscher%20et%20al.%202022%20-%20Neural%20representational%20geometry%20underlies%20few-shot%20concept%20learning.pdf)

> [!abstract] Abstract
> Understanding the neural basis of the remarkable human cognitive capacity to learn novel concepts from just one or a few sensory experiences constitutes a fundamental problem. We propose a simple, biologically plausible, mathematically tractable, and computationally powerful neural mechanism for few-shot learning of naturalistic concepts. We posit that the concepts that can be learned from few examples are defined by tightly circumscribed manifolds in the neural firing-rate space of higher-order sensory areas. We further posit that a single plastic downstream readout neuron learns to discriminate new concepts based on few examples using a simple plasticity rule. We demonstrate the computational power of our proposal by showing that it can achieve high few-shot learning accuracy on natural visual concepts using both macaque inferotemporal cortex representations and deep neural network (DNN) models of these representations and can even learn novel visual concepts specified only through linguistic descriptors. Moreover, we develop a mathematical theory of few-shot learning that links neurophysiology to predictions about behavioral outcomes by delineating several fundamental and measurable geometric properties of neural representations that can accurately predict the few-shot learning performance of naturalistic concepts across all our numerical simulations. This theory reveals, for instance, that high-dimensional manifolds enhance the ability to learn new concepts from few examples. Intriguingly, we observe striking mismatches between the geometry of manifolds in the primate visual pathway and in trained DNNs. We discuss testable predictions of our theory for psychophysics and neurophysiological experiments.

