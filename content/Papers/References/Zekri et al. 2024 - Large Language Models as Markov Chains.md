---
year: 2024
month: 10
day: 3
date: 3 October 2024
authors:
  - "Zekri, Oussama"
  - "Odonnat, Ambroise"
  - "Benechehab, Abdelhakim"
  - "Bleistein, Linus"
  - "Boull'e, Nicolas"
  - "Redko, I."
generated: true
key: PTJRG99N
version: 2259
itemType: conferencePaper
title: Large Language Models as Markov Chains
url: "https://www.semanticscholar.org/paper/Large-Language-Models-as-Markov-Chains-Zekri-Odonnat/0d5dc0baf12635df418d8ee11816f2956b002f64"
accessDate: "2024-11-25T13:18:48Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-11-25T13:18:48Z"
dateModified: "2024-11-25T13:18:53Z"
super_collections:
  - ERQKEKFA
filename: Zekri et al. 2024 - Large Language Models as Markov Chains
marker: "[ðŸ‡¿](zotero://select/library/items/PTJRG99N)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Large Language Models as Markov Chains

> [!example] File
> [Zekri et al. 2024 - Large Language Models as Markov Chains](Zekri%20et%20al.%202024%20-%20Large%20Language%20Models%20as%20Markov%20Chains.pdf)

> [!abstract] Abstract
> Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic autoregressive language models with vocabulary of size $T$ and context window of size $K$ and Markov chains defined on a finite state space of size $\mathcal{O}(T^K)$. We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.

