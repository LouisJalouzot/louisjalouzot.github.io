---
year: 2021
month: 6
day: 9
date: 2021-06-09
authors:
  - "Chen, Xingguang"
  - "Wang, Sibo"
generated: true
key: 3Y2YLDTE
version: 2264
itemType: journalArticle
title: Efficient Approximate Algorithms for Empirical Entropy and Mutual Information
publicationTitle: Proceedings of the 2021 International Conference on Management of Data
pages: 274-286
language: en
DOI: 10.1145/3448016.3457255
url: "https://dl.acm.org/doi/10.1145/3448016.3457255"
accessDate: "2025-01-06T14:37:11Z"
libraryCatalog: Semantic Scholar
extra: "Conference Name: SIGMOD/PODS '21: International Conference on Management of Data ISBN: 9781450383431 Place: Virtual Event China Publisher: ACM"
collections:
  - ERQKEKFA
dateAdded: "2025-01-06T14:37:11Z"
dateModified: "2025-01-06T14:37:32Z"
super_collections:
  - ERQKEKFA
filename: Chen and Wang 2021 - Efficient Approximate Algorithms for Empirical Entropy and Mutual Information
marker: "[ðŸ‡¿](zotero://select/library/items/3Y2YLDTE)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Efficient Approximate Algorithms for Empirical Entropy and Mutual Information

> [!example] File
> [Chen and Wang 2021 - Efficient Approximate Algorithms for Empirical Entropy and Mutual Information](Chen%20and%20Wang%202021%20-%20Efficient%20Approximate%20Algorithms%20for%20Empirical%20Entropy%20and%20Mutual%20Information.pdf)

> [!abstract] Abstract
> Empirical entropy is a classic concept in data mining and the foundation of many other important concepts like mutual information. However, computing the exact empirical entropy/mutual information on large datasets can be expensive. Some recent research work explores sampling techniques on the empirical entropy/mutual information to speed up the top-k and filtering queries. However, their solution still aims to return the exact answers to the queries, resulting in high computational costs. Motivated by this, in this work, we present approximate algorithms for the top-k queries and filtering queries on empirical entropy and empirical mutual information. The approximate algorithm allows user-specified tunable parameters to control the trade-off between the query efficiency and accuracy. We design effective stopping rules to return the approximate answers with improved query time. We further present theoretical analysis and show that our proposed solutions achieve improved time complexity over previous solutions. We experimentally evaluate our proposed algorithms on real datasets with up to 31M records and 179 attributes. Our experimental results show that the proposed algorithm consistently outperforms the state of the art in terms of computational efficiency, by an order of magnitude in most cases, while providing the same accurate result.

