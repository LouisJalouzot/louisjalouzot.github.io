---
year: 2020
month: 6
day: 27
date: 2020-06-27
authors:
  - "Schrimpf, Martin"
  - "Blank, Idan"
  - "Tuckute, Greta"
  - "Kauf, Carina"
  - "Hosseini, Eghbal A."
  - "Kanwisher, Nancy"
  - "Tenenbaum, Joshua"
  - "Fedorenko, Evelina"
generated: true
key: 5IT48HGD
version: 2237
itemType: journalArticle
title: "The neural architecture of language: Integrative modeling converges on predictive processing"
language: en
DOI: 10.1101/2020.06.26.174482
shortTitle: The neural architecture of language
url: "http://biorxiv.org/lookup/doi/10.1101/2020.06.26.174482"
accessDate: "2024-05-07T23:18:54Z"
libraryCatalog: Neuroscience
collections:
  - ERQKEKFA
dateAdded: "2024-05-07T23:18:54Z"
dateModified: "2024-05-07T23:19:03Z"
super_collections:
  - ERQKEKFA
filename: Schrimpf et al. 2020 - The neural architecture of language Integrative modeling converges on predictive processing
marker: "[ðŸ‡¿](zotero://select/library/items/5IT48HGD)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] The neural architecture of language: Integrative modeling converges on predictive processing

> [!example] File
> [Schrimpf et al. 2020 - The neural architecture of language Integrative modeling converges on predictive processing](Schrimpf%20et%20al.%202020%20-%20The%20neural%20architecture%20of%20language%20Integrative%20modeling%20converges%20on%20predictive%20processing.pdf)

> [!abstract] Abstract
> Abstract
>           The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a first systematic study taking this approach to higher-level cognition: human language processing, our speciesâ€™ signature cognitive skill. We find that the most powerful â€˜transformerâ€™ models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (fMRI, ECoG). Modelsâ€™ neural fits (â€˜brain scoreâ€™) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.
>           
>             Significance
>             Language is a quintessentially human ability. Research has long probed the functional architecture of language processing in the mind and brain using diverse brain imaging, behavioral, and computational modeling approaches. However, adequate neurally mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report an important first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurements â€“ providing computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.

