---
zoteroTags:
  - "#nosource"
  - Computer Science - Machine Learning
year: 2024
month: 2
day: 6
date: 2024-02-06
authors:
  - "Cui, Hugo"
  - "Behrens, Freya"
  - "Krzakala, Florent"
  - "ZdeborovÃ¡, Lenka"
generated: true
key: A7WZDSQ3
version: 2037
itemType: preprint
title: A phase transition between positional and semantic learning in a solvable model of dot-product attention
repository: arXiv
archiveID: "arXiv:2402.03902"
DOI: 10.48550/arXiv.2402.03902
url: "http://arxiv.org/abs/2402.03902"
accessDate: "2024-03-09T04:13:23Z"
libraryCatalog: arXiv.org
extra: "arXiv:2402.03902 [cs]"
collections:
  - ERQKEKFA
dateAdded: "2024-03-09T04:13:24Z"
dateModified: "2024-04-21T09:54:51Z"
super_collections:
  - ERQKEKFA
filename: Cui et al. 2024 - A phase transition between positional and semantic learning in a solvable model of dot-product attention
marker: "[ðŸ‡¿](zotero://select/library/items/A7WZDSQ3)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] A phase transition between positional and semantic learning in a solvable model of dot-product attention

> [!example] File
> [Cui et al. 2024 - A phase transition between positional and semantic learning in a solvable model of dot-product attention](Cui%20et%20al.%202024%20-%20A%20phase%20transition%20between%20positional%20and%20semantic%20learning%20in%20a%20solvable%20model%20of%20dot-product%20attention.pdf)

> [!abstract] Abstract
> We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.

