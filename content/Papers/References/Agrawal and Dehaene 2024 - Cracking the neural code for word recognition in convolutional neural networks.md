---
zoteroTags:
  - Computer Vision and Pattern Recognition (cs.CV)
  - "FOS: Biological sciences"
  - "FOS: Computer and information sciences"
  - Neurons and Cognition (q-bio.NC)
year: 2024
date: 2024
authors:
  - Agrawal, Aakash
  - Dehaene, Stanislas
generated: true
key: 25PT8KHC
version: 2255
itemType: journalArticle
title: Cracking the neural code for word recognition in convolutional neural networks
DOI: 10.48550/ARXIV.2403.06159
url: https://arxiv.org/abs/2403.06159
accessDate: 2024-11-06T10:33:35Z
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 2"
collections:
  - ERQKEKFA
dateAdded: 2024-11-06T10:33:35Z
dateModified: 2024-11-06T10:34:13Z
super_collections:
  - ERQKEKFA
filename: Agrawal and Dehaene 2024 - Cracking the neural code for word recognition in convolutional neural networks
marker: "[ðŸ‡¿](zotero://select/library/items/25PT8KHC)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Cracking the neural code for word recognition in convolutional neural networks

> [!example] File
> [Agrawal and Dehaene 2024 - Cracking the neural code for word recognition in convolutional neural networks](Agrawal%20and%20Dehaene%202024%20-%20Cracking%20the%20neural%20code%20for%20word%20recognition%20in%20convolutional%20neural%20networks.pdf)

> [!abstract] Abstract
> Learning to read places a strong challenge on the visual system. Years of expertise lead to a remarkable capacity to separate highly similar letters and encode their relative positions, thus distinguishing words such as FORM and FROM, invariantly over a large range of sizes and absolute positions. How neural circuits achieve invariant word recognition remains unknown. Here, we address this issue by training deep neural network models to recognize written words and then analyzing how reading-specialized units emerge and operate across different layers of the network. With literacy, a small subset of units becomes specialized for word recognition in the learned script, similar to the "visual word form area" of the human brain. We show that these units are sensitive to specific letter identities and their distance from the blank space at the left or right of a word, thus acting as "space bigrams". These units specifically encode ordinal positions and operate by pooling across low and high-frequency detector units from early layers of the network. The proposed neural code provides a mechanistic insight into how information on letter identity and position is extracted and allow for invariant word recognition, and leads to predictions for reading behavior, error patterns, and the neurophysiology of reading.

