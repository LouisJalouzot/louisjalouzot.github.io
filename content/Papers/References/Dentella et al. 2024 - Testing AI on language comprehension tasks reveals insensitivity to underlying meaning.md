---
year: 2024
month: 11
day: 14
date: 2024-11-14
authors:
  - "Dentella, Vittoria"
  - "GÃ¼nther, Fritz"
  - "Murphy, Elliot"
  - "Marcus, Gary"
  - "Leivada, Evelina"
generated: true
key: 8YGHPFGH
version: 2260
itemType: journalArticle
title: Testing AI on language comprehension tasks reveals insensitivity to underlying meaning
publicationTitle: Scientific Reports
volume: 14
issue: 1
pages: 28083
journalAbbreviation: Sci Rep
language: en
DOI: 10.1038/s41598-024-79531-8
ISSN: 2045-2322
url: "https://www.nature.com/articles/s41598-024-79531-8"
accessDate: "2024-11-29T12:21:08Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-11-29T12:21:08Z"
dateModified: "2024-11-29T12:21:27Z"
super_collections:
  - ERQKEKFA
filename: Dentella et al. 2024 - Testing AI on language comprehension tasks reveals insensitivity to underlying meaning
marker: "[ðŸ‡¿](zotero://select/library/items/8YGHPFGH)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Testing AI on language comprehension tasks reveals insensitivity to underlying meaning

> [!example] File
> [Dentella et al. 2024 - Testing AI on language comprehension tasks reveals insensitivity to underlying meaning](Dentella%20et%20al.%202024%20-%20Testing%20AI%20on%20language%20comprehension%20tasks%20reveals%20insensitivity%20to%20underlying%20meaning.pdf)

> [!abstract] Abstract
> Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education. Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning. Yet, reverse-engineering is bound by Moravecâ€™s Paradox, according to which easy skills are hard. We systematically assess 7 state-of-the-art models on a novel benchmark. Models answered a series of comprehension questions, each prompted multiple times in two settings, permitting one-word or open-length replies. Each question targets a short text featuring high-frequency linguistic constructions. To establish a baseline for achieving human-like performance, we tested 400 humans on the same prompts. Based on a dataset of nâ€‰=â€‰26,680 datapoints, we discovered that LLMs perform at chance accuracy and waver considerably in their answers. Quantitatively, the tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding. We interpret this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and we argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information.

