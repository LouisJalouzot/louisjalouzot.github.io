---
year: 2024
month: 10
day: 23
date: 23 October 2024
authors:
  - "Smolensky, P."
  - "Fernandez, Roland"
  - "Zhou, Zhenghao Herbert"
  - "Opper, Mattia"
  - "Gao, Jianfeng"
generated: true
key: U52Z33TJ
version: 2254
itemType: conferencePaper
title: Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks
url: "https://www.semanticscholar.org/paper/Mechanisms-of-Symbol-Processing-for-In-Context-in-Smolensky-Fernandez/2c2f17ac628b241091c2057739aa408287e2956d"
accessDate: "2024-10-28T08:39:36Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-10-28T08:39:36Z"
dateModified: "2024-11-29T13:13:31Z"
super_collections:
  - ERQKEKFA
filename: Smolensky et al. 2024 - Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks
marker: "[ðŸ‡¿](zotero://select/library/items/U52Z33TJ)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks

> [!example] File
> [Smolensky et al. 2024 - Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks](Smolensky%20et%20al.%202024%20-%20Mechanisms%20of%20Symbol%20Processing%20for%20In-Context%20Learning%20in%20Transformer%20Networks.pdf)

> [!abstract] Abstract
> Large Language Models (LLMs) have demonstrated impressive abilities in symbol processing through in-context learning (ICL). This success flies in the face of decades of predictions that artificial neural networks cannot master abstract symbol manipulation. We seek to understand the mechanisms that can enable robust symbol processing in transformer networks, illuminating both the unanticipated success, and the significant limitations, of transformers in symbol processing. Borrowing insights from symbolic AI on the power of Production System architectures, we develop a high-level language, PSL, that allows us to write symbolic programs to do complex, abstract symbol processing, and create compilers that precisely implement PSL programs in transformer networks which are, by construction, 100% mechanistically interpretable. We demonstrate that PSL is Turing Universal, so the work can inform the understanding of transformer ICL in general. The type of transformer architecture that we compile from PSL programs suggests a number of paths for enhancing transformers' capabilities at symbol processing. (Note: The first section of the paper gives an extended synopsis of the entire paper.)

