---
zoteroTags:
  - Learning algorithms
  - Machine learning
  - Network models
year: 2024
month: 1
date: 2024-02-01
authors:
  - "Song, Yuhang"
  - "Millidge, Beren"
  - "Salvatori, Tommaso"
  - "Lukasiewicz, Thomas"
  - "Xu, Zhenghua"
  - "Bogacz, Rafal"
generated: true
key: DWMJVQQV
version: 2622
itemType: journalArticle
paperTitle: Inferring neural activity before plasticity as a foundation for learning beyond backpropagation
publicationTitle: Nature Neuroscience
volume: 27
issue: 2
pages: 348-358
journalAbbreviation: Nat Neurosci
language: en
DOI: 10.1038/s41593-023-01514-1
ISSN: 1546-1726
url: "https://www.nature.com/articles/s41593-023-01514-1"
accessDate: "2025-05-23T13:55:20Z"
libraryCatalog: www.nature.com
rights: 2024 The Author(s)
extra: "Publisher: Nature Publishing Group"
dateAdded: "2025-05-23T13:55:20Z"
dateModified: "2025-05-23T13:55:20Z"
filename: Song et al. 2024 - Inferring neural activity before plasticity as a foundation for learning beyond backpropagation.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/DWMJVQQV)"
publish: true
type: reference
---
# Inferring neural activity before plasticity as a foundation for learning beyond backpropagation

[PDF file](/Papers/PDFs/Song%20et%20al.%202024%20-%20Inferring%20neural%20activity%20before%20plasticity%20as%20a%20foundation%20for%20learning%20beyond%20backpropagation.pdf)

> [!abstract] Abstract
> For both humans and machines, the essence of learning is to pinpoint which components in its information processing pipeline are responsible for an error in its output, a challenge that is known as â€˜credit assignmentâ€™. It has long been assumed that credit assignment is best solved by backpropagation, which is also the foundation of modern machine learning. Here, we set out a fundamentally different principle on credit assignment called â€˜prospective configurationâ€™. In prospective configuration, the network first infers the pattern of neural activity that should result from learning, and then the synaptic weights are modified to consolidate the change in neural activity. We demonstrate that this distinct mechanism, in contrast to backpropagation, (1) underlies learning in a well-established family of models of cortical circuits, (2) enables learning that is more efficient and effective in many contexts faced by biological organisms and (3) reproduces surprising patterns of neural activity and behavior observed in diverse human and rat learning experiments.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

