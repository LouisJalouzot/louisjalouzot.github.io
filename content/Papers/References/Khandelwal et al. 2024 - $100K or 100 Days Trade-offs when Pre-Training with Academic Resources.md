---
year: 2024
month: 10
day: 30
date: 30 October 2024
authors:
  - "Khandelwal, Apoorv"
  - "Yun, Tian"
  - "Nayak, Nihal V."
  - "Merullo, Jack"
  - "Bach, Stephen H."
  - "Sun, Chen"
  - "Pavlick, Ellie"
generated: true
key: 2KNEN4RC
version: 2258
itemType: conferencePaper
title: "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources"
shortTitle: $100K or 100 Days
url: "https://www.semanticscholar.org/paper/%24100K-or-100-Days%3A-Trade-offs-when-Pre-Training-Khandelwal-Yun/f61e081dc57d4fbf75977f7293627d5a402c0301"
accessDate: "2024-11-25T09:05:28Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-11-25T09:05:28Z"
dateModified: "2024-11-25T09:13:53Z"
super_collections:
  - ERQKEKFA
filename: Khandelwal et al. 2024 - $100K or 100 Days Trade-offs when Pre-Training with Academic Resources
marker: "[ðŸ‡¿](zotero://select/library/items/2KNEN4RC)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] $100K or 100 Days: Trade-offs when Pre-Training with Academic Resources

> [!example] File
> [Khandelwal et al. 2024 - $100K or 100 Days Trade-offs when Pre-Training with Academic Resources](Khandelwal%20et%20al.%202024%20-%20$100K%20or%20100%20Days%20Trade-offs%20when%20Pre-Training%20with%20Academic%20Resources.pdf)

> [!abstract] Abstract
> Pre-training is notoriously compute-intensive and academic researchers are notoriously under-resourced. It is, therefore, commonly assumed that academics can't pre-train models. In this paper, we seek to clarify this assumption. We first survey academic researchers to learn about their available compute and then empirically measure the time to replicate models on such resources. We introduce a benchmark to measure the time to pre-train models on given GPUs and also identify ideal settings for maximizing training speed. We run our benchmark on a range of models and academic GPUs, spending 2,000 GPU-hours on our experiments. Our results reveal a brighter picture for academic pre-training: for example, although Pythia-1B was originally trained on 64 GPUs for 3 days, we find it is also possible to replicate this model (with the same hyper-parameters) in 3x fewer GPU-days: i.e. on 4 GPUs in 18 days. We conclude with a cost-benefit analysis to help clarify the trade-offs between price and pre-training time. We believe our benchmark will help academic researchers conduct experiments that require training larger models on more data. We fully release our codebase at: https://github.com/apoorvkh/academic-pretraining.

