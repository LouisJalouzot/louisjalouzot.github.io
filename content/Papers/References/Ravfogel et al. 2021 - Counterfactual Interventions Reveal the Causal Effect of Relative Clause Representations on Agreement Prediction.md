---
zoteroTags:
  - Computer Science - Computation and Language
year: 2021
month: 9
day: 15
date: 2021-09-15
authors:
  - "Ravfogel, Shauli"
  - "Prasad, Grusha"
  - "Linzen, Tal"
  - "Goldberg, Yoav"
generated: true
key: 6VN622XT
version: 2244
itemType: preprint
title: Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction
repository: arXiv
archiveID: "arXiv:2105.06965"
DOI: 10.48550/arXiv.2105.06965
url: "http://arxiv.org/abs/2105.06965"
accessDate: "2024-03-09T03:57:42Z"
libraryCatalog: arXiv.org
extra: "arXiv:2105.06965 [cs]"
collections:
  - ERQKEKFA
dateAdded: "2024-03-09T03:57:42Z"
dateModified: "2024-03-09T03:57:51Z"
super_collections:
  - ERQKEKFA
filename: Ravfogel et al. 2021 - Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction
marker: "[ðŸ‡¿](zotero://select/library/items/6VN622XT)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction

> [!example] File
> [Ravfogel et al. 2021 - Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction](Ravfogel%20et%20al.%202021%20-%20Counterfactual%20Interventions%20Reveal%20the%20Causal%20Effect%20of%20Relative%20Clause%20Representations%20on%20Agreement%20Prediction.pdf)

> [!abstract] Abstract
> When language models process syntactically complex sentences, do they use their representations of syntax in a manner that is consistent with the grammar of the language? We propose AlterRep, an intervention-based method to address this question. For any linguistic feature of a given sentence, AlterRep generates counterfactual representations by altering how the feature is encoded, while leaving intact all other aspects of the original representation. By measuring the change in a model's word prediction behavior when these counterfactual representations are substituted for the original ones, we can draw conclusions about the causal effect of the linguistic feature in question on the model's behavior. We apply this method to study how BERT models of different sizes process relative clauses (RCs). We find that BERT variants use RC boundary information during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category.

