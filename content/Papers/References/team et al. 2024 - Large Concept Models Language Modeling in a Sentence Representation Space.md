---
year: 2024
month: 12
day: 11
date: 11 December 2024
authors:
  - "team, The Lcm"
  - "Barrault, L."
  - "Duquenne, Paul-Ambroise"
  - "Elbayad, Maha"
  - "Kozhevnikov, Artyom"
  - "Alastruey, Belen"
  - "Andrews, Pierre"
  - "Coria, Mariano"
  - "Couairon, Guillaume"
  - "Costa-jussÃ , M."
  - "Dale, David"
  - "ElSahar, Hady"
  - "Heffernan, Kevin"
  - "Janeiro, Joao Maria"
  - "Tran, Tuan"
  - "Ropers, C."
  - "S'anchez, Eduardo"
  - "Roman, Robin San"
  - "Mourachko, Alex"
  - "Saleem, Safiyyah"
  - "Schwenk, Holger"
generated: true
key: APTYULG7
version: 2263
itemType: conferencePaper
title: "Large Concept Models: Language Modeling in a Sentence Representation Space"
shortTitle: Large Concept Models
url: "https://www.semanticscholar.org/paper/Large-Concept-Models%3A-Language-Modeling-in-a-Space-team-Barrault/ec81cfb3c35321a7aa1e4c0dfd1bbcf7c4b97ded"
accessDate: "2024-12-24T14:36:31Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-12-24T14:36:31Z"
dateModified: "2024-12-24T14:37:14Z"
super_collections:
  - ERQKEKFA
filename: team et al. 2024 - Large Concept Models Language Modeling in a Sentence Representation Space
marker: "[ðŸ‡¿](zotero://select/library/items/APTYULG7)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Large Concept Models: Language Modeling in a Sentence Representation Space

> [!example] File
> [team et al. 2024 - Large Concept Models Language Modeling in a Sentence Representation Space](team%20et%20al.%202024%20-%20Large%20Concept%20Models%20Language%20Modeling%20in%20a%20Sentence%20Representation%20Space.pdf)

> [!abstract] Abstract
> LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for many tasks. The current established technology of LLMs is to process input and generate output at the token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well beyond single words, to analyze information and to generate creative content. In this paper, we present an attempt at an architecture which operates on an explicit higher-level semantic representation, which we name a concept. Concepts are language- and modality-agnostic and represent a higher level idea or action in a flow. Hence, we build a"Large Concept Model". In this study, as proof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence embedding space, SONAR, which supports up to 200 languages in both text and speech modalities. The Large Concept Model is trained to perform autoregressive sentence prediction in an embedding space. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation, and models operating in a quantized SONAR space. These explorations are performed using 1.6B parameter models and training data in the order of 1.3T tokens. We then scale one architecture to a model size of 7B parameters and training data of about 2.7T tokens. We perform an experimental evaluation on several generative tasks, namely summarization and a new task of summary expansion. Finally, we show that our model exhibits impressive zero-shot generalization performance to many languages, outperforming existing LLMs of the same size. The training code of our models is freely available.

