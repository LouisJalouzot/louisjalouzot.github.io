---
zoteroTags:
  - Artificial Intelligence (cs.AI)
  - "FOS: Computer and information sciences"
year: 2023
date: 2023-01-01
authors:
  - "Geiger, Atticus"
  - "Wu, Zhengxuan"
  - "Potts, Christopher"
  - "Icard, Thomas"
  - "Goodman, Noah D."
generated: true
key: 9GSSYJM3
version: 2555
itemType: journalArticle
paperTitle: Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations
DOI: 10.48550/ARXIV.2303.02536
url: "https://arxiv.org/abs/2303.02536"
accessDate: "2025-04-24T09:02:39Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 4"
dateAdded: "2025-04-24T09:02:39Z"
dateModified: "2025-04-24T09:02:39Z"
filename: Geiger et al. 2023 - Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/9GSSYJM3)"
publish: true
type: reference
---
# Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations

[PDF file](/Papers/PDFs/Geiger%20et%20al.%202023%20-%20Finding%20Alignments%20Between%20Interpretable%20Causal%20Variables%20and%20Distributed%20Neural%20Representations.pdf)

> [!abstract] Abstract
> Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to conducting causal abstraction analyses and allows us to find conceptual structure in trained neural nets.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

