---
year: 2020
month: 7
day: 4
date: 2020-07-04
authors:
  - "Caucheteux, Charlotte"
  - "King, Jean-Remi"
generated: true
key: HHCY9ET9
version: 2237
itemType: journalArticle
title: "Language processing in brains and deep neural networks: computational convergence and its limits"
language: en
DOI: 10.1101/2020.07.03.186288
shortTitle: Language processing in brains and deep neural networks
url: "http://biorxiv.org/lookup/doi/10.1101/2020.07.03.186288"
accessDate: "2024-05-07T23:17:35Z"
libraryCatalog: Neuroscience
collections:
  - ERQKEKFA
dateAdded: "2024-05-07T23:17:35Z"
dateModified: "2024-05-07T23:17:41Z"
super_collections:
  - ERQKEKFA
filename: Caucheteux and King 2020 - Language processing in brains and deep neural networks computational convergence and its limits
marker: "[ðŸ‡¿](zotero://select/library/items/HHCY9ET9)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Language processing in brains and deep neural networks: computational convergence and its limits

> [!example] File
> [Caucheteux and King 2020 - Language processing in brains and deep neural networks computational convergence and its limits](Caucheteux%20and%20King%202020%20-%20Language%20processing%20in%20brains%20and%20deep%20neural%20networks%20computational%20convergence%20and%20its%20limits.pdf)

> [!abstract] Abstract
> Deep Learning has recently led to major advances in natural language processing. Do these models process sentences similarly to humans, and is this similarity driven by specific principles? Using a variety of artificial neural networks, trained on image classification, word embedding, or language modeling, we evaluate whether their architectural and functional properties lead them to generate activations linearly comparable to those of 102 human brains measured with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We show that image, word and contextualized word embeddings separate the hierarchical levels of language processing in the brain. Critically, we compare \NNetworks{} embeddings in their ability to linearly map onto these brain responses. The results show that (1) the position of the layer in the network and (2) the ability of the network to accurately predict words from context are the main factors responsible for the emergence of brain-like representations in artificial neural networks. Together, these results show how perceptual, lexical and compositional representations precisely unfold within each cortical region and contribute to uncovering the governing principles of language processing in brains and algorithms.

