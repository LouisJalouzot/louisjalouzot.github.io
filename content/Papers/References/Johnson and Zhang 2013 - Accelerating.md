---
year: 2013
month: 11
day: 5
date: 2013-12-04
authors:
  - "Johnson, Rie"
  - "Zhang, Tong"
generated: true
key: 5VNBV6TU
version: 2409
itemType: conferencePaper
paperTitle: Accelerating Stochastic Gradient Descent using Predictive Variance Reduction
conferenceName: Neural Information Processing Systems
url: "https://www.semanticscholar.org/paper/Accelerating-Stochastic-Gradient-Descent-using-Johnson-Zhang/43c05444fbc239321f6676f3cd539cac34fde7b8"
accessDate: "2025-03-21T09:50:12Z"
libraryCatalog: Semantic Scholar
dateAdded: "2025-03-21T09:50:12Z"
dateModified: "2025-03-21T09:50:12Z"
filename: Johnson and Zhang 2013 - Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/5VNBV6TU)"
publish: true
type: reference
---
# Accelerating Stochastic Gradient Descent using Predictive Variance Reduction

[PDF file](/Papers/PDFs/Johnson%20and%20Zhang%202013%20-%20Accelerating%20Stochastic%20Gradient%20Descent%20using%20Predictive%20Variance%20Reduction.pdf)

> [!abstract] Abstract
> Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

