---
zoteroTags:
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
  - Machine Learning (cs.LG)
year: 2024
date: 2024
authors:
  - "Xiao, Bushi"
  - "Gao, Chao"
  - "Zhang, Demi"
generated: true
key: 54VWUBQC
version: 2245
itemType: journalArticle
title: "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming"
DOI: 10.48550/ARXIV.2405.09508
shortTitle: Modeling Bilingual Sentence Processing
url: "https://arxiv.org/abs/2405.09508"
accessDate: "2024-07-05T07:01:18Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 1"
collections:
  - ERQKEKFA
dateAdded: "2024-07-05T07:01:18Z"
dateModified: "2024-07-05T07:01:26Z"
super_collections:
  - ERQKEKFA
filename: Xiao et al. 2024 - Modeling Bilingual Sentence Processing Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming
marker: "[ðŸ‡¿](zotero://select/library/items/54VWUBQC)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming

> [!example] File
> [Xiao et al. 2024 - Modeling Bilingual Sentence Processing Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming](Xiao%20et%20al.%202024%20-%20Modeling%20Bilingual%20Sentence%20Processing%20Evaluating%20RNN%20and%20Transformer%20Architectures%20for%20Cross-Language%20Structural%20Priming.pdf)

> [!abstract] Abstract
> This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer in replicating cross-language structural priming: a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Additionally, we utilize large language models (LLM) to measure the cross-lingual structural priming effect. Our findings indicate that Transformer outperform RNN in generating primed sentence structures, challenging the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggesting a role for cue-based retrieval mechanisms. Overall, this work contributes to our understanding of how computational models may reflect human cognitive processes in multilingual contexts.

