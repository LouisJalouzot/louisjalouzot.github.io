---
year: 2014
month: 6
day: 1
date: 2014-06-30
authors:
  - "Defazio, Aaron"
  - "Bach, F."
  - "Lacoste-Julien, Simon"
generated: true
key: 5GVH3G6H
version: 2413
itemType: conferencePaper
paperTitle: "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives"
conferenceName: Neural Information Processing Systems
shortTitle: SAGA
url: "https://www.semanticscholar.org/paper/SAGA%3A-A-Fast-Incremental-Gradient-Method-With-for-Defazio-Bach/4daec165c1f4aa1206b0d91c0b26f0287d1ef52d"
accessDate: "2025-03-21T09:52:24Z"
libraryCatalog: Semantic Scholar
dateAdded: "2025-03-21T09:52:24Z"
dateModified: "2025-03-21T09:52:24Z"
filename: Defazio et al. 2014 - SAGA A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/5GVH3G6H)"
publish: true
type: reference
---
# SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives

[PDF file](/Papers/PDFs/Defazio%20et%20al.%202014%20-%20SAGA%20A%20Fast%20Incremental%20Gradient%20Method%20With%20Support%20for%20Non-Strongly%20Convex%20Composite%20Objectives.pdf)

> [!abstract] Abstract
> In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

