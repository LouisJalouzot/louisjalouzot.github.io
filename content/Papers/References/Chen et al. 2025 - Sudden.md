---
zoteroTags:
  - Computer Science - Computation and Language
year: 2025
month: 2
day: 21
date: 2025-03-21
authors:
  - "Chen, Angelica"
  - "Shwartz-Ziv, Ravid"
  - "Cho, Kyunghyun"
  - "Leavitt, Matthew L."
  - "Saphra, Naomi"
generated: true
key: T94GUNY4
version: 2727
itemType: preprint
paperTitle: "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs"
repository: arXiv
archiveID: "arXiv:2309.07311"
DOI: 10.48550/arXiv.2309.07311
url: "http://arxiv.org/abs/2309.07311"
accessDate: "2025-06-23T07:16:12Z"
shortTitle: Sudden Drops in the Loss
libraryCatalog: arXiv.org
extra: "arXiv:2309.07311 [cs]"
dateAdded: "2025-06-23T07:16:12Z"
dateModified: "2025-06-23T07:16:12Z"
filename: Chen et al. 2025 - Sudden Drops in the Loss Syntax Acquisition Phase Transitions and Simplicity Bias in MLMs.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/T94GUNY4)"
publish: true
type: reference
---
# Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs

[PDF file](/Papers/PDFs/Chen%20et%20al.%202025%20-%20Sudden%20Drops%20in%20the%20Loss%20Syntax%20Acquisition%20Phase%20Transitions%20and%20Simplicity%20Bias%20in%20MLMs.pdf)

> [!abstract] Abstract
> Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

