---
zoteroTags:
  - Artificial Intelligence (cs.AI)
  - "FOS: Computer and information sciences"
  - Machine Learning (cs.LG)
  - Neural and Evolutionary Computing (cs.NE)
year: 2024
date: 2024-01-01
authors:
  - "Csord치s, R칩bert"
  - "Potts, Christopher"
  - "Manning, Christopher D."
  - "Geiger, Atticus"
generated: true
key: C4GNSWKG
version: 2363
itemType: journalArticle
paperTitle: Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations
DOI: 10.48550/ARXIV.2408.10920
url: "https://arxiv.org/abs/2408.10920"
accessDate: "2025-03-13T16:34:08Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 1"
dateAdded: "2025-03-13T16:34:08Z"
dateModified: "2025-03-13T16:34:08Z"
filename: Csord치s et al. 2024 - Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations.pdf
marker: "[游쯓(zotero://select/library/items/C4GNSWKG)"
publish: true
type: reference
---
# Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations

[PDF file](/Papers/PDFs/Csord치s%20et%20al.%202024%20-%20Recurrent%20Neural%20Networks%20Learn%20to%20Store%20and%20Generate%20Sequences%20using%20Non-Linear%20Representations.pdf)

> [!abstract] Abstract
> The Linear Representation Hypothesis (LRH) states that neural networks learn to encode concepts as directions in activation space, and a strong version of the LRH states that models learn only such encodings. In this paper, we present a counterexample to this strong LRH: when trained to repeat an input token sequence, gated recurrent neural networks (RNNs) learn to represent the token at each position with a particular order of magnitude, rather than a direction. These representations have layered features that are impossible to locate in distinct linear subspaces. To show this, we train interventions to predict and manipulate tokens by learning the scaling factor corresponding to each sequence position. These interventions indicate that the smallest RNNs find only this magnitude-based solution, while larger RNNs have linear representations. These findings strongly indicate that interpretability research should not be confined by the LRH.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

