---
year: 2023
month: 10
day: 5
date: 2023-10-05
authors:
  - "DÃ©fossez, Alexandre"
  - "Caucheteux, Charlotte"
  - "Rapin, JÃ©rÃ©my"
  - "Kabeli, Ori"
  - "King, Jean-RÃ©mi"
generated: true
key: ZZIUBXYQ
version: 2247
itemType: journalArticle
title: Decoding speech perception from non-invasive brain recordings
publicationTitle: Nature Machine Intelligence
volume: 5
issue: 10
pages: 1097-1107
journalAbbreviation: Nat Mach Intell
language: en
DOI: 10.1038/s42256-023-00714-5
ISSN: 2522-5839
url: "https://www.nature.com/articles/s42256-023-00714-5"
accessDate: "2024-09-16T09:45:28Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-09-16T09:45:28Z"
dateModified: "2024-09-16T09:45:36Z"
super_collections:
  - ERQKEKFA
filename: DÃ©fossez et al. 2023 - Decoding speech perception from non-invasive brain recordings
marker: "[ðŸ‡¿](zotero://select/library/items/ZZIUBXYQ)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Decoding speech perception from non-invasive brain recordings

> [!example] File
> [DÃ©fossez et al. 2023 - Decoding speech perception from non-invasive brain recordings](DÃ©fossez%20et%20al.%202023%20-%20Decoding%20speech%20perception%20from%20non-invasive%20brain%20recordings.pdf)

> [!abstract] Abstract
> Abstract
>             Decoding speech from brain activity is a long-awaited goal in both healthcare and neuroscience. Invasive devices have recently led to major milestones in this regard: deep-learning algorithms trained on intracranial recordings can now start to decode elementary linguistic features such as letters, words and audio-spectrograms. However, extending this approach to natural speech and non-invasive brain recordings remains a major challenge. Here we introduce a model trained with contrastive learning to decode self-supervised representations of perceived speech from the non-invasive recordings of a large cohort of healthy individuals. To evaluate this approach, we curate and integrate four public datasets, encompassing 175 volunteers recorded with magneto-encephalography or electro-encephalography while they listened to short stories and isolated sentences. The results show that our model can identify, from 3â€‰seconds of magneto-encephalography signals, the corresponding speech segment with up to 41% accuracy out of more than 1,000 distinct possibilities on average across participants, and with up to 80% in the best participantsâ€”a performance that allows the decoding of words and phrases absent from the training set. The comparison of our model with a variety of baselines highlights the importance of a contrastive objective, pretrained representations of speech and a common convolutional architecture simultaneously trained across multiple participants. Finally, the analysis of the decoderâ€™s predictions suggests that they primarily depend on lexical and contextual semantic representations. Overall, this effective decoding of perceived speech from non-invasive recordings delineates a promising path to decode language from brain activity, without putting patients at risk of brain surgery.

