---
year: 2020
month: 10
day: 13
date: 13 October 2020
authors:
  - "Kim, Sungnyun"
  - "Lee, Gihun"
  - "Bae, Sangmin"
  - "Yun, Seyoung"
generated: true
key: J4DHR4LD
version: 2233
itemType: journalArticle
title: "MixCo: Mix-up Contrastive Learning for Visual Representation"
publicationTitle: ArXiv
shortTitle: MixCo
url: "https://www.semanticscholar.org/paper/MixCo%3A-Mix-up-Contrastive-Learning-for-Visual-Kim-Lee/3021152ab7540da7fd85baf2560568d8ef4a9b23"
accessDate: "2024-05-31T21:07:01Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-05-31T21:07:01Z"
dateModified: "2024-05-31T21:07:14Z"
super_collections:
  - ERQKEKFA
filename: Kim et al. 2020 - MixCo Mix-up Contrastive Learning for Visual Representation
marker: "[ðŸ‡¿](zotero://select/library/items/J4DHR4LD)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] MixCo: Mix-up Contrastive Learning for Visual Representation

> [!example] File
> [Kim et al. 2020 - MixCo Mix-up Contrastive Learning for Visual Representation](Kim%20et%20al.%202020%20-%20MixCo%20Mix-up%20Contrastive%20Learning%20for%20Visual%20Representation.pdf)

> [!abstract] Abstract
> Contrastive learning has shown remarkable results in recent self-supervised approaches for visual representation. By learning to contrast positive pairs' representation from the corresponding negatives pairs, one can train good visual representations without human annotations. This paper proposes Mix-up Contrast (MixCo), which extends the contrastive learning concept to semi-positives encoded from the mix-up of positive and negative images. MixCo aims to learn the relative similarity of representations, reflecting how much the mixed images have the original positives. We validate the efficacy of MixCo when applied to the recent self-supervised learning algorithms under the standard linear evaluation protocol on TinyImageNet, CIFAR10, and CIFAR100. In the experiments, MixCo consistently improves test accuracy. Remarkably, the improvement is more significant when the learning capacity (e.g., model size) is limited, suggesting that MixCo might be more useful in real-world scenarios. The code is available at: https://github.com/Lee-Gihun/MixCo-Mixup-Contrast.

