---
year: 2019
month: 7
day: 25
date: 2019-07-25
authors:
  - "Wang, Chi"
  - "Ding, Bailu"
generated: true
key: JTLYJTBT
version: 2263
itemType: journalArticle
title: Fast Approximation of Empirical Entropy via Subsampling
publicationTitle: "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
pages: 658-667
language: en
DOI: 10.1145/3292500.3330938
url: "https://dl.acm.org/doi/10.1145/3292500.3330938"
accessDate: "2025-01-06T14:33:51Z"
libraryCatalog: Semantic Scholar
extra: "Conference Name: KDD '19: The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ISBN: 9781450362016 Place: Anchorage AK USA Publisher: ACM"
collections:
  - ERQKEKFA
dateAdded: "2025-01-06T14:33:51Z"
dateModified: "2025-01-06T14:34:07Z"
super_collections:
  - ERQKEKFA
filename: Wang and Ding 2019 - Fast Approximation of Empirical Entropy via Subsampling
marker: "[ðŸ‡¿](zotero://select/library/items/JTLYJTBT)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Fast Approximation of Empirical Entropy via Subsampling

> [!example] File
> [Wang and Ding 2019 - Fast Approximation of Empirical Entropy via Subsampling](Wang%20and%20Ding%202019%20-%20Fast%20Approximation%20of%20Empirical%20Entropy%20via%20Subsampling.pdf)

> [!abstract] Abstract
> Empirical entropy refers to the information entropy calculated from the empirical distribution of a dataset. It is a widely used aggregation function for knowledge discovery, as well as the foundation of other aggregation functions such as mutual information. However, computing the exact empirical entropy on a large-scale dataset can be expensive. Using a random subsample, we can compute an approximation of the empirical entropy efficiently. We derive probabilistic error bounds for the approximation, where the error bounds reduce in a near square root rate with respect to the subsample size. We further study two applications which can benefit from the error-bounded approximation: feature ranking and filtering based on mutual information. We develop algorithms to progressively subsample the dataset and return correct answers with high probability. The sample complexity of the algorithms is independent of data size. The empirical evaluation of our algorithms on large-scale real-world datasets demonstrates up to three orders of magnitude speedup over exact methods with \errrate\ error.

