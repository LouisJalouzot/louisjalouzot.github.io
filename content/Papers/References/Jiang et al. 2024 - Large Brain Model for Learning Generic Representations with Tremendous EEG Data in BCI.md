---
zoteroTags:
  - "FOS: Computer and information sciences"
  - Machine Learning (cs.LG)
year: 2024
date: 2024
authors:
  - "Jiang, Wei-Bang"
  - "Zhao, Li-Ming"
  - "Lu, Bao-Liang"
generated: true
key: J3XBJDGC
version: 2258
itemType: journalArticle
title: Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI
DOI: 10.48550/ARXIV.2405.18765
url: "https://arxiv.org/abs/2405.18765"
accessDate: "2024-11-25T09:12:58Z"
libraryCatalog: Semantic Scholar
rights: "arXiv.org perpetual, non-exclusive license"
extra: "Publisher: arXiv Version Number: 1"
collections:
  - ERQKEKFA
dateAdded: "2024-11-25T09:12:58Z"
dateModified: "2024-11-25T09:13:47Z"
super_collections:
  - ERQKEKFA
filename: Jiang et al. 2024 - Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI
marker: "[ðŸ‡¿](zotero://select/library/items/J3XBJDGC)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI

> [!example] File
> [Jiang et al. 2024 - Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI](Jiang%20et%20al.%202024%20-%20Large%20Brain%20Model%20for%20Learning%20Generic%20Representations%20with%20Tremendous%20EEG%20Data%20in%20BCI.pdf)

> [!abstract] Abstract
> The current electroencephalogram (EEG) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction (BCI), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models (LLMs) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large EEG Models (LEMs). We hope that LEMs can break through the limitations of different task types of EEG datasets, and obtain universal perceptual capabilities of EEG signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of EEG datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for EEG called Large Brain Model (LaBraM). LaBraM enables cross-dataset learning by segmenting the EEG signals into EEG channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw EEG channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked EEG channel patches. The LaBraMs were pre-trained on about 2,500 hours of various types of EEG signals from around 20 datasets and validated on multiple different types of downstream tasks. Experiments on abnormal detection, event type classification, emotion recognition, and gait prediction show that our LaBraM outperforms all compared SOTA methods in their respective fields. Our code is available at https://github.com/935963004/LaBraM.

