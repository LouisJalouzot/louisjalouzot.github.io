---
year: 2024
date: 2024
authors:
  - "Rodriguez, Amilleah"
  - "Wang, Shaonan"
  - "Pylkkänen, L."
generated: true
key: GD8IJKAR
version: 2235
itemType: conferencePaper
title: "Do Neural Language Models Inferentially Compose Concepts the Way Humans Can?"
conferenceName: International Conference on Language Resources and Evaluation
url: "https://www.semanticscholar.org/paper/Do-Neural-Language-Models-Inferentially-Compose-the-Rodriguez-Wang/36a7b34b3887d10add71588b11f5c3d5928aef5b"
accessDate: "2024-05-31T19:56:18Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-05-31T19:56:18Z"
dateModified: "2024-05-31T19:56:24Z"
super_collections:
  - ERQKEKFA
filename: Rodriguez et al. 2024 - Do Neural Language Models Inferentially Compose Concepts the Way Humans Can
marker: "[🇿](zotero://select/library/items/GD8IJKAR)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Do Neural Language Models Inferentially Compose Concepts the Way Humans Can?

> [!example] File
> [Rodriguez et al. 2024 - Do Neural Language Models Inferentially Compose Concepts the Way Humans Can](Rodriguez%20et%20al.%202024%20-%20Do%20Neural%20Language%20Models%20Inferentially%20Compose%20Concepts%20the%20Way%20Humans%20Can.pdf)

> [!abstract] Abstract
> While compositional interpretation is the core of language understanding, humans also derive meaning via inference. For example, while the phrase “the blue hat” introduces a blue hat into the discourse via the direct composition of “blue” and “hat,” the same discourse entity is introduced by the phrase “the blue color of this hat” despite the absence of any local composition between “blue” and “hat.” Instead, we infer that if the color is blue and it belongs to the hat, the hat must be blue. We tested the performance of neural language models and humans on such inferentially driven conceptual compositions, eliciting probability estimates for a noun in a minimally composed phrase, “This blue hat”, following contexts that had introduced the conceptual combinations of those nouns and adjectives either syntactically or inferentially. Surprisingly, our findings reveal significant disparities between the performance of neural language models and human judgments. Among the eight models evaluated, RoBERTa, BERT-large, and GPT-2 exhibited the closest resemblance to human responses, while other models faced challenges in accurately identifying compositions in the provided contexts. Our study reveals that language models and humans may rely on different approaches to represent and compose lexical items across sentence structure. All data and code are accessible at https://github.com/wangshaonan/BlueHat.

