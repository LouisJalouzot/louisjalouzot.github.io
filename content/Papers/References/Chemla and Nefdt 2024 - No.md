---
zoteroTags:
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
year: 2024
date: 2024-01-01
authors:
  - "Chemla, Emmanuel"
  - "Nefdt, Ryan M."
generated: true
key: E3AESEMW
version: 2611
itemType: journalArticle
paperTitle: "No Such Thing as a General Learner: Language models and their dual optimization"
DOI: 10.48550/ARXIV.2408.09544
shortTitle: No Such Thing as a General Learner
url: "https://arxiv.org/abs/2408.09544"
accessDate: "2025-05-18T14:29:58Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 2"
dateAdded: "2025-05-18T14:29:58Z"
dateModified: "2025-05-18T14:29:58Z"
filename: Chemla and Nefdt 2024 - No Such Thing as a General Learner Language models and their dual optimization.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/E3AESEMW)"
publish: true
type: reference
---
# No Such Thing as a General Learner: Language models and their dual optimization

[PDF file](/Papers/PDFs/Chemla%20and%20Nefdt%202024%20-%20No%20Such%20Thing%20as%20a%20General%20Learner%20Language%20models%20and%20their%20dual%20optimization.pdf)

> [!abstract] Abstract
> What role can the otherwise successful Large Language Models (LLMs) play in the understanding of human cognition, and in particular in terms of informing language acquisition debates? To contribute to this question, we first argue that neither humans nor LLMs are general learners, in a variety of senses. We make a novel case for how in particular LLMs follow a dual-optimization process: they are optimized during their training (which is typically compared to language acquisition), and modern LLMs have also been selected, through a process akin to natural selection in a species. From this perspective, we argue that the performance of LLMs, whether similar or dissimilar to that of humans, does not weigh easily on important debates about the importance of human cognitive biases for language.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

