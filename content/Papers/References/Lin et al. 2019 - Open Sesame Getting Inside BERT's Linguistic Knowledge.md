---
zoteroTags:
  - Computer Science - Computation and Language
year: 2019
month: 6
day: 4
date: 2019-06-04
authors:
  - "Lin, Yongjie"
  - "Tan, Yi Chern"
  - "Frank, Robert"
generated: true
key: N2TA3UBH
version: 2037
itemType: preprint
title: "Open Sesame: Getting Inside BERT's Linguistic Knowledge"
repository: arXiv
archiveID: "arXiv:1906.01698"
DOI: 10.48550/arXiv.1906.01698
url: "http://arxiv.org/abs/1906.01698"
accessDate: "2024-03-28T16:03:52Z"
shortTitle: Open Sesame
libraryCatalog: arXiv.org
extra: "arXiv:1906.01698 [cs]"
collections:
  - ERQKEKFA
dateAdded: "2024-03-28T16:03:52Z"
dateModified: "2024-03-28T16:07:07Z"
super_collections:
  - ERQKEKFA
filename: Lin et al. 2019 - Open Sesame Getting Inside BERT's Linguistic Knowledge
marker: "[ðŸ‡¿](zotero://select/library/items/N2TA3UBH)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Open Sesame: Getting Inside BERT's Linguistic Knowledge

> [!example] File
> [Lin et al. 2019 - Open Sesame Getting Inside BERT's Linguistic Knowledge](Lin%20et%20al.%202019%20-%20Open%20Sesame%20Getting%20Inside%20BERT's%20Linguistic%20Knowledge.pdf)

> [!abstract] Abstract
> How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT's representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT's representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT's representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.

