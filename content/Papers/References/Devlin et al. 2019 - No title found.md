---
year: 2019
date: 2019
authors:
  - "Devlin, Jacob"
  - "Chang, Ming-Wei"
  - "Lee, Kenton"
  - "Toutanova, Kristina"
generated: true
key: NRIARFY8
version: 1783
itemType: conferencePaper
title: "[No title found]"
proceedingsTitle: Proceedings of the 2019 Conference of the North
conferenceName: Proceedings of the 2019 Conference of the North
place: "Minneapolis, Minnesota"
publisher: Association for Computational Linguistics
pages: 4171-4186
language: en
DOI: 10.18653/v1/N19-1423
url: "http://aclweb.org/anthology/N19-1423"
accessDate: "2025-02-19T20:55:24Z"
libraryCatalog: Semantic Scholar
deleted: 1
dateAdded: "2025-02-19T20:55:24Z"
dateModified: "2025-02-19T20:55:33Z"
filename: Devlin et al. 2019 - No title found
marker: "[ðŸ‡¿](zotero://select/library/items/NRIARFY8)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] [No title found]

> [!example] File
> [Devlin et al. 2019 - No title found](Devlin%20et%20al.%202019%20-%20No%20title%20found.pdf)

> [!abstract] Abstract
> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

