---
year: 2020
month: 11
day: 9
date: 9 November 2020
authors:
  - "Tamkin, Alex"
  - "Jurafsky, Dan"
  - "Goodman, Noah D."
generated: true
key: B2Z3KF8S
version: 2250
itemType: journalArticle
title: "Language Through a Prism: A Spectral Approach for Multiscale Language Representations"
publicationTitle: ArXiv
shortTitle: Language Through a Prism
url: "https://www.semanticscholar.org/paper/Language-Through-a-Prism%3A-A-Spectral-Approach-for-Tamkin-Jurafsky/38d5e7774e79861315e043dc2dd764d051516d74"
accessDate: "2024-10-07T08:22:21Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-10-07T08:22:21Z"
dateModified: "2024-10-07T08:22:32Z"
super_collections:
  - ERQKEKFA
filename: Tamkin et al. 2020 - Language Through a Prism A Spectral Approach for Multiscale Language Representations
marker: "[ðŸ‡¿](zotero://select/library/items/B2Z3KF8S)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Language Through a Prism: A Spectral Approach for Multiscale Language Representations

> [!example] File
> [Tamkin et al. 2020 - Language Through a Prism A Spectral Approach for Multiscale Language Representations](Tamkin%20et%20al.%202020%20-%20Language%20Through%20a%20Prism%20A%20Spectral%20Approach%20for%20Multiscale%20Language%20Representations.pdf)

> [!abstract] Abstract
> Language exhibits structure at different scales, ranging from subwords to words, sentences, paragraphs, and documents. To what extent do deep models capture information at these scales, and can we force them to better capture structure across this hierarchy? We approach this question by focusing on individual neurons, analyzing the behavior of their activations at different timescales. We show that signal processing provides a natural framework for separating structure across scales, enabling us to 1) disentangle scale-specific information in existing embeddings and 2) train models to learn more about particular scales. Concretely, we apply spectral filters to the activations of a neuron across an input, producing filtered embeddings that perform well on part of speech tagging (word-level), dialog speech acts classification (utterance-level), or topic classification (document-level), while performing poorly on the other tasks. We also present a prism layer for training models, which uses spectral filters to constrain different neurons to model structure at different scales. Our proposed BERT + Prism model can better predict masked tokens using long-range context and produces multiscale representations that perform better at utterance- and document-level tasks. Our methods are general and readily applicable to other domains besides language, such as images, audio, and video.

