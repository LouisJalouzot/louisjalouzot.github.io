---
zoteroTags:
  - Artificial Intelligence (cs.AI)
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
  - Symbolic Computation (cs.SC)
year: 2023
date: 2023
authors:
  - "Wong, Lionel"
  - "Grand, Gabriel"
  - "Lew, Alexander K."
  - "Goodman, Noah D."
  - "Mansinghka, Vikash K."
  - "Andreas, Jacob"
  - "Tenenbaum, Joshua B."
generated: true
key: BQIRBM8K
version: 2266
itemType: journalArticle
title: "From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought"
DOI: 10.48550/ARXIV.2306.12672
shortTitle: From Word Models to World Models
url: "https://arxiv.org/abs/2306.12672"
accessDate: "2025-01-18T11:25:39Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 2"
collections:
  - ERQKEKFA
dateAdded: "2025-01-18T11:25:39Z"
dateModified: "2025-01-18T11:27:34Z"
super_collections:
  - ERQKEKFA
filename: Wong et al. 2023 - From Word Models to World Models Translating from Natural Language to the Probabilistic Language of Thought
marker: "[ðŸ‡¿](zotero://select/library/items/BQIRBM8K)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought

> [!example] File
> [Wong et al. 2023 - From Word Models to World Models Translating from Natural Language to the Probabilistic Language of Thought](Wong%20et%20al.%202023%20-%20From%20Word%20Models%20to%20World%20Models%20Translating%20from%20Natural%20Language%20to%20the%20Probabilistic%20Language%20of%20Thought.pdf)

> [!abstract] Abstract
> How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.

