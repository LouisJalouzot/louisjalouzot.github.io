---
year: 2022
date: 2022
authors:
  - "Ni, Jianmo"
  - "Hernandez Abrego, Gustavo"
  - "Constant, Noah"
  - "Ma, Ji"
  - "Hall, Keith"
  - "Cer, Daniel"
  - "Yang, Yinfei"
generated: true
key: VRES8IAI
version: 2255
itemType: journalArticle
title: "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models"
publicationTitle: "Findings of the Association for Computational Linguistics: ACL 2022"
pages: 1864-1874
language: en
DOI: 10.18653/v1/2022.findings-acl.146
shortTitle: Sentence-T5
url: "https://aclanthology.org/2022.findings-acl.146"
accessDate: "2024-10-31T16:54:18Z"
libraryCatalog: Semantic Scholar
extra: "Conference Name: Findings of the Association for Computational Linguistics: ACL 2022 Place: Dublin, Ireland Publisher: Association for Computational Linguistics"
collections:
  - ERQKEKFA
dateAdded: "2024-10-31T16:54:18Z"
dateModified: "2024-10-31T16:55:15Z"
super_collections:
  - ERQKEKFA
filename: Ni et al. 2022 - Sentence-T5 Scalable Sentence Encoders from Pre-trained Text-to-Text Models
marker: "[ðŸ‡¿](zotero://select/library/items/VRES8IAI)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models

> [!example] File
> [Ni et al. 2022 - Sentence-T5 Scalable Sentence Encoders from Pre-trained Text-to-Text Models](Ni%20et%20al.%202022%20-%20Sentence-T5%20Scalable%20Sentence%20Encoders%20from%20Pre-trained%20Text-to-Text%20Models.pdf)

> [!abstract] Abstract
> We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.

