---
zoteroTags:
  - Computer Science - Computation and Language
  - Computer Science - Machine Learning
year: 2024
month: 12
day: 28
date: 2024-12-28
authors:
  - "Park, Core Francisco"
  - "Lubana, Ekdeep Singh"
  - "Pres, Itamar"
  - "Tanaka, Hidenori"
generated: true
key: WWBCHP5J
version: 2047
itemType: preprint
title: Competition Dynamics Shape Algorithmic Phases of In-Context Learning
repository: arXiv
archiveID: "arXiv:2412.01003"
DOI: 10.48550/arXiv.2412.01003
url: "http://arxiv.org/abs/2412.01003"
accessDate: "2025-02-23T21:13:36Z"
libraryCatalog: arXiv.org
extra: "arXiv:2412.01003 [cs]"
collections:
  - ERQKEKFA
dateAdded: "2025-02-23T21:13:36Z"
dateModified: "2025-02-23T21:13:36Z"
super_collections:
  - ERQKEKFA
filename: Park et al. 2024 - Competition Dynamics Shape Algorithmic Phases of In-Context Learning
marker: "[ðŸ‡¿](zotero://select/library/items/WWBCHP5J)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Competition Dynamics Shape Algorithmic Phases of In-Context Learning

> [!example] File
> [Park et al. 2024 - Competition Dynamics Shape Algorithmic Phases of In-Context Learning](Park%20et%20al.%202024%20-%20Competition%20Dynamics%20Shape%20Algorithmic%20Phases%20of%20In-Context%20Learning.pdf)

> [!abstract] Abstract
> In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model's behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competition dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.

