---
zoteroTags:
  - Computer Science - Computation and Language
  - Computer Science - Machine Learning
  - I.2
  - Quantitative Biology - Biomolecules
  - notion
year: 2021
month: 3
day: 28
date: 2021-03-28
authors:
  - "Vig, Jesse"
  - "Madani, Ali"
  - "Varshney, Lav R."
  - "Xiong, Caiming"
  - "Socher, Richard"
  - "Rajani, Nazneen Fatema"
generated: true
key: 7L7ECSEZ
version: 2037
itemType: preprint
title: "BERTology Meets Biology: Interpreting Attention in Protein Language Models"
repository: arXiv
archiveID: "arXiv:2006.15222"
DOI: 10.48550/arXiv.2006.15222
url: "http://arxiv.org/abs/2006.15222"
accessDate: "2024-03-28T16:11:20Z"
shortTitle: BERTology Meets Biology
libraryCatalog: arXiv.org
extra: "arXiv:2006.15222 [cs, q-bio]"
collections:
  - ERQKEKFA
dateAdded: "2024-03-28T16:11:20Z"
dateModified: "2024-03-28T16:11:24Z"
super_collections:
  - ERQKEKFA
filename: Vig et al. 2021 - BERTology Meets Biology Interpreting Attention in Protein Language Models
marker: "[ðŸ‡¿](zotero://select/library/items/7L7ECSEZ)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] BERTology Meets Biology: Interpreting Attention in Protein Language Models

> [!example] File
> [Vig et al. 2021 - BERTology Meets Biology Interpreting Attention in Protein Language Models](Vig%20et%20al.%202021%20-%20BERTology%20Meets%20Biology%20Interpreting%20Attention%20in%20Protein%20Language%20Models.pdf)

> [!abstract] Abstract
> Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at https://github.com/salesforce/provis.

