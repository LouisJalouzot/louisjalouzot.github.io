---
zoteroTags:
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
year: 2023
date: 2023-01-01
authors:
  - "Wu, Zhengxuan"
  - "Geiger, Atticus"
  - "Icard, Thomas"
  - "Potts, Christopher"
  - "Goodman, Noah D."
generated: true
key: MW9GDW9S
version: 2564
itemType: journalArticle
paperTitle: "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca"
DOI: 10.48550/ARXIV.2305.08809
shortTitle: Interpretability at Scale
url: "https://arxiv.org/abs/2305.08809"
accessDate: "2025-05-07T11:37:55Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 3"
dateAdded: "2025-05-07T11:37:55Z"
dateModified: "2025-05-07T11:37:55Z"
filename: Wu et al. 2023 - Interpretability at Scale Identifying Causal Mechanisms in Alpaca.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/MW9GDW9S)"
publish: true
type: reference
---
# Interpretability at Scale: Identifying Causal Mechanisms in Alpaca

[PDF file](/Papers/PDFs/Wu%20et%20al.%202023%20-%20Interpretability%20at%20Scale%20Identifying%20Causal%20Mechanisms%20in%20Alpaca.pdf)

> [!abstract] Abstract
> Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that has uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call Boundless DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply Boundless DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With Boundless DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward faithfully understanding the inner-workings of our ever-growing and most widely deployed language models. Our tool is extensible to larger LLMs and is released publicly at `https://github.com/stanfordnlp/pyvene`.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

