---
year: 2022
month: 6
day: 9
date: 2022-06-09
authors:
  - "Kumar, Sreejan"
  - "Sumers, Theodore R."
  - "Yamakoshi, Takateru"
  - "Goldstein, Ariel"
  - "Hasson, Uri"
  - "Norman, Kenneth A."
  - "Griffiths, Thomas L."
  - "Hawkins, Robert D."
  - "Nastase, Samuel A."
generated: true
key: IGSSCNX2
version: 2246
itemType: journalArticle
title: Shared functional specialization in transformer-based language models and the human brain
language: en
DOI: 10.1101/2022.06.08.495348
url: "http://biorxiv.org/lookup/doi/10.1101/2022.06.08.495348"
accessDate: "2024-07-16T19:58:13Z"
libraryCatalog: Neuroscience
collections:
  - ERQKEKFA
dateAdded: "2024-07-16T19:58:13Z"
dateModified: "2024-07-16T19:58:19Z"
super_collections:
  - ERQKEKFA
filename: Kumar et al. 2022 - Shared functional specialization in transformer-based language models and the human brain
marker: "[üáø](zotero://select/library/items/IGSSCNX2)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Shared functional specialization in transformer-based language models and the human brain

> [!example] File
> [Kumar et al. 2022 - Shared functional specialization in transformer-based language models and the human brain](Kumar%20et%20al.%202022%20-%20Shared%20functional%20specialization%20in%20transformer-based%20language%20models%20and%20the%20human%20brain.pdf)

> [!abstract] Abstract
> Abstract
>           
>             Humans use complex linguistic structures to transmit ideas to one another. The brain is thought to deploy specialized computations to process these structures. Recently, a new class of artificial neural networks based on the Transformer architecture has revolutionized the field of language modeling, attracting attention from neuroscientists seeking to understand the neurobiology of language
>             in silico
>             . Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. Prior work has focused on the internal representations (the ‚Äúembeddings‚Äù) generated by these circuits. In this paper, we instead analyze the circuit computations directly: we deconstruct these computations into functionally-specialized ‚Äútransformations‚Äù to provide a complementary window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we first verify that the transformations account for considerable variance in brain activity across the cortical language network. We then demonstrate that the emergent syntactic computations performed by individual, functionally-specialized ‚Äúattention heads‚Äù differentially predict brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings indicate that large language models and the cortical language network may converge on similar trends of functional specialization for processing natural language.

