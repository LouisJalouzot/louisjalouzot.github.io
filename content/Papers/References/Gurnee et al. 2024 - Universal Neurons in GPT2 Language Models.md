---
zoteroTags:
  - Artificial Intelligence (cs.AI)
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
  - Machine Learning (cs.LG)
year: 2024
date: 2024
authors:
  - "Gurnee, Wes"
  - "Horsley, Theo"
  - "Guo, Zifan Carl"
  - "Kheirkhah, Tara Rezaei"
  - "Sun, Qinyi"
  - "Hathaway, Will"
  - "Nanda, Neel"
  - "Bertsimas, Dimitris"
generated: true
key: IJYQP2VG
version: 2235
itemType: journalArticle
title: Universal Neurons in GPT2 Language Models
DOI: 10.48550/ARXIV.2401.12181
url: "https://arxiv.org/abs/2401.12181"
accessDate: "2024-05-31T20:06:18Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 1"
collections:
  - ERQKEKFA
dateAdded: "2024-05-31T20:06:18Z"
dateModified: "2024-05-31T20:06:33Z"
super_collections:
  - ERQKEKFA
filename: Gurnee et al. 2024 - Universal Neurons in GPT2 Language Models
marker: "[ðŸ‡¿](zotero://select/library/items/IJYQP2VG)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Universal Neurons in GPT2 Language Models

> [!example] File
> [Gurnee et al. 2024 - Universal Neurons in GPT2 Language Models](Gurnee%20et%20al.%202024%20-%20Universal%20Neurons%20in%20GPT2%20Language%20Models.pdf)

> [!abstract] Abstract
> A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.

