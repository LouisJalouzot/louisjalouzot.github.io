---
year: 2019
date: 2019
authors:
  - "Devlin, Jacob"
  - "Chang, Ming-Wei"
  - "Lee, Kenton"
  - "Toutanova, Kristina"
generated: true
key: EN2V8BHK
version: 2255
itemType: conferencePaper
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
proceedingsTitle: Proceedings of the 2019 Conference of the North
conferenceName: Proceedings of the 2019 Conference of the North
place: "Minneapolis, Minnesota"
publisher: Association for Computational Linguistics
pages: 4171-4186
language: en
DOI: 10.18653/v1/N19-1423
url: "http://aclweb.org/anthology/N19-1423"
accessDate: "2024-11-05T10:50:39Z"
libraryCatalog: Semantic Scholar
extra: "TLDR: A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
collections:
  - ERQKEKFA
dateAdded: "2024-11-05T10:50:39Z"
dateModified: "2025-02-25T10:50:15Z"
super_collections:
  - ERQKEKFA
filename: Devlin et al. 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding
marker: "[ðŸ‡¿](zotero://select/library/items/EN2V8BHK)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

> [!example] File
> [Devlin et al. 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding](Devlin%20et%20al.%202019%20-%20BERT%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf)

> [!abstract] Abstract
> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

