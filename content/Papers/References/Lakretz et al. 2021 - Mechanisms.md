---
zoteroTags:
  - Grammatical agreement
  - Language models
  - Long-range dependencies
  - Recurrent neural networks
  - Recursion
  - Relative clauses
  - Syntactic processing
year: 2021
month: 7
day: 1
date: 2021-08-01
authors:
  - "Lakretz, Yair"
  - "Hupkes, Dieuwke"
  - "Vergallito, Alessandra"
  - "Marelli, Marco"
  - "Baroni, Marco"
  - "Dehaene, Stanislas"
generated: true
key: VJXJHTWA
version: 2641
itemType: journalArticle
paperTitle: Mechanisms for handling nested dependencies in neural-network language models and humans
publicationTitle: Cognition
volume: 213
pages: 104699
series: "Special Issue in Honour of Jacques Mehler, Cognition’s founding editor"
journalAbbreviation: Cognition
DOI: 10.1016/j.cognition.2021.104699
ISSN: 0010-0277
url: "https://www.sciencedirect.com/science/article/pii/S0010027721001189"
accessDate: "2025-06-01T20:04:01Z"
libraryCatalog: ScienceDirect
dateAdded: "2025-06-01T20:04:01Z"
dateModified: "2025-06-01T20:04:01Z"
filename: Lakretz et al. 2021 - Mechanisms for handling nested dependencies in neural-network language models and humans.pdf
marker: "[🇿](zotero://select/library/items/VJXJHTWA)"
publish: true
type: reference
---
# Mechanisms for handling nested dependencies in neural-network language models and humans

[PDF file](/Papers/PDFs/Lakretz%20et%20al.%202021%20-%20Mechanisms%20for%20handling%20nested%20dependencies%20in%20neural-network%20language%20models%20and%20humans.pdf)

> [!abstract] Abstract
> Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanisms remain largely unknown. We studied whether a modern artificial neural network trained with “deep learning” methods mimics a central aspect of human sentence processing, namely the storing of grammatical number and gender information in working memory and its use in long-distance agreement (e.g., capturing the correct number agreement between subject and verb when they are separated by other phrases). Although the network, a recurrent architecture with Long Short-Term Memory units, was solely trained to predict the next word in a large corpus, analysis showed the emergence of a very sparse set of specialized units that successfully handled local and long-distance syntactic agreement for grammatical number. However, the simulations also showed that this mechanism does not support full recursion and fails with some long-range embedded dependencies. We tested the model's predictions in a behavioral experiment where humans detected violations in number agreement in sentences with systematic variations in the singular/plural status of multiple nouns, with or without embedding. Human and model error patterns were remarkably similar, showing that the model echoes various effects observed in human data. However, a key difference was that, with embedded long-range dependencies, humans remained above chance level, while the model's systematic errors brought it below chance. Overall, our study shows that exploring the ways in which modern artificial neural networks process sentences leads to precise and testable hypotheses about human linguistic performance.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

