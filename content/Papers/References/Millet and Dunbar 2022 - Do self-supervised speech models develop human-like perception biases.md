---
zoteroTags:
  - Audio and Speech Processing (eess.AS)
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
  - "FOS: Electrical engineering, electronic engineering, information engineering"
  - Sound (cs.SD)
year: 2022
date: 2022
authors:
  - "Millet, Juliette"
  - "Dunbar, Ewan"
generated: true
key: 2NTJ5629
version: 2248
itemType: journalArticle
title: "Do self-supervised speech models develop human-like perception biases?"
DOI: 10.48550/ARXIV.2205.15819
url: "https://arxiv.org/abs/2205.15819"
accessDate: "2024-09-21T09:24:16Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 1"
collections:
  - ERQKEKFA
dateAdded: "2024-09-21T09:24:16Z"
dateModified: "2024-09-21T09:24:22Z"
super_collections:
  - ERQKEKFA
filename: Millet and Dunbar 2022 - Do self-supervised speech models develop human-like perception biases
marker: "[ðŸ‡¿](zotero://select/library/items/2NTJ5629)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Do self-supervised speech models develop human-like perception biases?

> [!example] File
> [Millet and Dunbar 2022 - Do self-supervised speech models develop human-like perception biases](Millet%20and%20Dunbar%202022%20-%20Do%20self-supervised%20speech%20models%20develop%20human-like%20perception%20biases.pdf)

> [!abstract] Abstract
> Self-supervised models for speech processing form representational spaces without using any external labels. Increasingly, they appear to be a feasible way of at least partially eliminating costly manual annotations, a problem of particular concern for low-resource languages. But what kind of representational spaces do these models construct? Human perception specializes to the sounds of listeners' native languages. Does the same thing happen in self-supervised models? We examine the representational spaces of three kinds of state-of-the-art self-supervised models: wav2vec 2.0, HuBERT and contrastive predictive coding (CPC), and compare them with the perceptual spaces of French-speaking and English-speaking human listeners, both globally and taking account of the behavioural differences between the two language groups. We show that the CPC model shows a small native language effect, but that wav2vec 2.0 and HuBERT seem to develop a universal speech perception space which is not language specific. A comparison against the predictions of supervised phone recognisers suggests that all three self-supervised models capture relatively fine-grained perceptual phenomena, while supervised models are better at capturing coarser, phone-level, effects of listeners' native language, on perception.

