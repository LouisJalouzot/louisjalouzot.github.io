---
zoteroTags:
  - Dynamical Systems (math.DS)
  - "FOS: Biological sciences"
  - "FOS: Computer and information sciences"
  - "FOS: Mathematics"
  - Machine Learning (cs.LG)
  - Machine Learning (stat.ML)
  - Neural and Evolutionary Computing (cs.NE)
  - Neurons and Cognition (q-bio.NC)
year: 2023
date: 2023
authors:
  - "Pellegrino, Arthur"
  - "Cayco-Gajic, N Alex"
  - "Chadwick, Angus"
generated: true
key: V2ZLRHXE
version: 2039
itemType: journalArticle
title: Low Tensor Rank Learning of Neural Dynamics
DOI: 10.48550/ARXIV.2308.11567
url: "https://arxiv.org/abs/2308.11567"
accessDate: "2024-10-18T11:14:25Z"
libraryCatalog: Semantic Scholar
rights: "arXiv.org perpetual, non-exclusive license"
extra: "Publisher: arXiv Version Number: 2"
collections:
  - ERQKEKFA
dateAdded: "2024-10-18T11:14:25Z"
dateModified: "2024-10-18T11:17:39Z"
super_collections:
  - ERQKEKFA
filename: Pellegrino et al. 2023 - Low Tensor Rank Learning of Neural Dynamics
marker: "[ðŸ‡¿](zotero://select/library/items/V2ZLRHXE)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Low Tensor Rank Learning of Neural Dynamics

> [!example] File
> [Pellegrino et al. 2023 - Low Tensor Rank Learning of Neural Dynamics](Pellegrino%20et%20al.%202023%20-%20Low%20Tensor%20Rank%20Learning%20of%20Neural%20Dynamics.pdf)

> [!abstract] Abstract
> Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide insight on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent dynamics from large-scale neural recordings.

