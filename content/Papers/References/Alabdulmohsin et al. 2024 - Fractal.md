---
zoteroTags:
  - Computer Science - Artificial Intelligence
  - Computer Science - Computation and Language
year: 2024
month: 4
day: 22
date: 2024-05-22
authors:
  - "Alabdulmohsin, Ibrahim"
  - "Tran, Vinh Q."
  - "Dehghani, Mostafa"
generated: true
key: 74SLQFDL
version: 2603
itemType: preprint
paperTitle: Fractal Patterns May Illuminate the Success of Next-Token Prediction
repository: arXiv
archiveID: "arXiv:2402.01825"
DOI: 10.48550/arXiv.2402.01825
url: "http://arxiv.org/abs/2402.01825"
accessDate: "2025-05-18T08:59:15Z"
libraryCatalog: arXiv.org
extra: "arXiv:2402.01825 [cs]"
dateAdded: "2025-05-18T08:59:16Z"
dateModified: "2025-05-18T08:59:16Z"
filename: Alabdulmohsin et al. 2024 - Fractal Patterns May Illuminate the Success of Next-Token Prediction.pdf
marker: "[ðŸ‡¿](zotero://select/library/items/74SLQFDL)"
publish: true
type: reference
---
# Fractal Patterns May Illuminate the Success of Next-Token Prediction

[PDF file](/Papers/PDFs/Alabdulmohsin%20et%20al.%202024%20-%20Fractal%20Patterns%20May%20Illuminate%20the%20Success%20of%20Next-Token%20Prediction.pdf)

> [!abstract] Abstract
> We study the fractal structure of language, aiming to provide a precise formalism for quantifying properties that may have been previously suspected but not formally shown. We establish that language is: (1) self-similar, exhibiting complexities at all levels of granularity, with no particular characteristic context length, and (2) long-range dependent (LRD), with a Hurst parameter of approximately H=0.7. Based on these findings, we argue that short-term patterns/dependencies in language, such as in paragraphs, mirror the patterns/dependencies over larger scopes, like entire documents. This may shed some light on how next-token prediction can capture the structure of text across multiple levels of granularity, from words and clauses to broader contexts and intents. In addition, we carry out an extensive analysis across different domains and architectures, showing that fractal parameters are robust. Finally, we demonstrate that the tiny variations in fractal parameters seen across LLMs improve upon perplexity-based bits-per-byte (BPB) in predicting their downstream performance. We hope these findings offer a fresh perspective on language and the mechanisms underlying the success of LLMs.

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it.

