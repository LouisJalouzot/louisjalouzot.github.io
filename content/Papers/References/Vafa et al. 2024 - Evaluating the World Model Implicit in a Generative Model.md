---
zoteroTags:
  - Artificial Intelligence (cs.AI)
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
year: 2024
date: 2024
authors:
  - "Vafa, Keyon"
  - "Chen, Justin Y."
  - "Kleinberg, Jon"
  - "Mullainathan, Sendhil"
  - "Rambachan, Ashesh"
generated: true
key: Y6682U4Q
version: 2253
itemType: journalArticle
title: Evaluating the World Model Implicit in a Generative Model
DOI: 10.48550/ARXIV.2406.03689
url: "https://arxiv.org/abs/2406.03689"
accessDate: "2024-10-25T09:30:49Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 2"
collections:
  - ERQKEKFA
dateAdded: "2024-10-25T09:30:49Z"
dateModified: "2024-10-25T09:33:09Z"
super_collections:
  - ERQKEKFA
filename: Vafa et al. 2024 - Evaluating the World Model Implicit in a Generative Model
marker: "[ðŸ‡¿](zotero://select/library/items/Y6682U4Q)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Evaluating the World Model Implicit in a Generative Model

> [!example] File
> [Vafa et al. 2024 - Evaluating the World Model Implicit in a Generative Model](Vafa%20et%20al.%202024%20-%20Evaluating%20the%20World%20Model%20Implicit%20in%20a%20Generative%20Model.pdf)

> [!abstract] Abstract
> Recent work suggests that large language models may implicitly learn world models. How should we assess this possibility? We formalize this question for the case where the underlying reality is governed by a deterministic finite automaton. This includes problems as diverse as simple logical reasoning, geographic navigation, game-playing, and chemistry. We propose new evaluation metrics for world model recovery inspired by the classic Myhill-Nerode theorem from language theory. We illustrate their utility in three domains: game playing, logic puzzles, and navigation. In all domains, the generative models we consider do well on existing diagnostics for assessing world models, but our evaluation metrics reveal their world models to be far less coherent than they appear. Such incoherence creates fragility: using a generative model to solve related but subtly different tasks can lead it to fail badly. Building generative models that meaningfully capture the underlying logic of the domains they model would be immensely valuable; our results suggest new ways to assess how close a given model is to that goal.

