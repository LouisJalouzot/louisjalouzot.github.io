---
zoteroTags:
  - Computation and Language (cs.CL)
  - "FOS: Computer and information sciences"
year: 2024
date: 2024
authors:
  - "Salle, Jeanne"
  - "Jalouzot, Louis"
  - "Lan, Nur"
  - "Chemla, Emmanuel"
  - "Lakretz, Yair"
generated: true
key: H2DH3HCR
version: 2247
itemType: journalArticle
title: "What Makes Two Language Models Think Alike?"
DOI: 10.48550/ARXIV.2406.12620
url: "https://arxiv.org/abs/2406.12620"
accessDate: "2024-08-01T10:02:40Z"
libraryCatalog: Semantic Scholar
rights: Creative Commons Attribution 4.0 International
extra: "Publisher: arXiv Version Number: 2"
collections:
  - ERQKEKFA
dateAdded: "2024-08-01T10:02:40Z"
dateModified: "2024-08-01T10:03:00Z"
super_collections:
  - ERQKEKFA
filename: Salle et al. 2024 - What Makes Two Language Models Think Alike
marker: "[ðŸ‡¿](zotero://select/library/items/H2DH3HCR)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] What Makes Two Language Models Think Alike?

> [!example] File
> [Salle et al. 2024 - What Makes Two Language Models Think Alike](Salle%20et%20al.%202024%20-%20What%20Makes%20Two%20Language%20Models%20Think%20Alike.pdf)

> [!abstract] Abstract
> Do architectural differences significantly affect the way models represent and process language? We propose a new approach, based on metric-learning encoding models (MLEMs), as a first step to answer this question. The approach provides a feature-based comparison of how any two layers of any two models represent linguistic information. We apply the method to BERT, GPT-2 and Mamba. Unlike previous methods, MLEMs offer a transparent comparison, by identifying the specific linguistic features responsible for similarities and differences. More generally, the method uses formal, symbolic descriptions of a domain, and use these to compare neural representations. As such, the approach can straightforwardly be extended to other domains, such as speech and vision, and to other neural systems, including human brains.

