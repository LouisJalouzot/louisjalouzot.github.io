---
year: 2023
month: 12
day: 19
date: 19 December 2023
authors:
  - "Yu, Zeping"
  - "Ananiadou, Sophia"
generated: true
key: CV763ZVG
version: 2247
itemType: conferencePaper
title: Neuron-Level Knowledge Attribution in Large Language Models
url: "https://www.semanticscholar.org/paper/Neuron-Level-Knowledge-Attribution-in-Large-Models-Yu-Ananiadou/9af3c1be7a4cfd38f10b9373e4623f4b64d467cd"
accessDate: "2024-07-20T19:31:40Z"
libraryCatalog: Semantic Scholar
collections:
  - ERQKEKFA
dateAdded: "2024-07-20T19:31:40Z"
dateModified: "2024-07-20T19:31:44Z"
super_collections:
  - ERQKEKFA
filename: Yu and Ananiadou 2023 - Neuron-Level Knowledge Attribution in Large Language Models
marker: "[ðŸ‡¿](zotero://select/library/items/CV763ZVG)"
---

>[!warning] Warning
> This note should not be modified as it can be overwritten by the plugin which generated it

> [!title] Neuron-Level Knowledge Attribution in Large Language Models

> [!example] File
> [Yu and Ananiadou 2023 - Neuron-Level Knowledge Attribution in Large Language Models](Yu%20and%20Ananiadou%202023%20-%20Neuron-Level%20Knowledge%20Attribution%20in%20Large%20Language%20Models.pdf)

> [!abstract] Abstract
> Identifying important neurons for final predictions is essential for understanding the mechanisms of large language models. Due to computational constraints, current attribution techniques struggle to operate at neuron level. In this paper, we propose a static method for pinpointing significant neurons for different outputs. Compared to seven other methods, our approach demonstrates superior performance across three metrics. Additionally, since most static methods typically only identify"value neurons"directly contributing to the final prediction, we introduce a static method for identifying"query neurons"which activate these"value neurons". Finally, we apply our methods to analyze the localization of six distinct types of knowledge across both attention and feed-forward network (FFN) layers. Our method and analysis are helpful for understanding the mechanisms of knowledge storage and set the stage for future research in knowledge editing. We will release our data and code on github.

