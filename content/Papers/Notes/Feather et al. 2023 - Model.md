---
title: Feather et al. 2023 - Model
paperTitle: "Model metamers reveal divergent invariances between biological and artificial neural networks"
authors: Feather, Jenelle,Leclerc, Guillaume,Mądry, Aleksander,McDermott, Josh H.
publish: true
cssclasses:
  - list-cards
type: annotation
project:
tags:
status: to read
progress: to annotate
---
# Annotation for [Feather et al. 2023 - Model](Papers/References/Feather%20et%20al.%202023%20-%20Model)

> [!abstract] Model metamers reveal divergent invariances between biological and artificial neural networks

> [!example]- Authors
> - [Feather, Jenelle](Feather%2C%20Jenelle)
> - [Leclerc, Guillaume](Leclerc%2C%20Guillaume)
> - [Mądry, Aleksander](M%C4%85dry%2C%20Aleksander)
> - [McDermott, Josh H.](McDermott%2C%20Josh%20H.)

**Year:** 2023
**DOI:** 10.1038/s41593-023-01442-0
**URL:** https://www.nature.com/articles/s41593-023-01442-0
**PDF:** [Feather et al. 2023 - Model](Papers/PDFs/Feather%20et%20al.%202023%20-%20Model%20metamers%20reveal%20divergent%20invariances%20between%20biological%20and%20artificial%20neural%20networks.pdf)

# Highlights


# Goal (yellow)


# Method (purple)


# Data (purple)


# Results (red)


# Discussion (blue)


# Questions


# AI Summary
## Model Metamers Reveal Divergent Invariances Between Biological and Artificial Neural Networks

This paper introduces "model metamers" as a novel method to probe the invariances learned by deep neural networks (DNNs) and compare them to human perceptual invariances in both visual and auditory domains. The core finding is that while DNNs are increasingly powerful models of sensory systems, their internal invariances often diverge significantly from those of humans, even for models that perform well on traditional brain-based benchmarks.

### Key Concepts and Methodology

- **Model Metamers:** These are stimuli synthesized to produce nearly identical activation patterns within a specific stage of a computational model as a natural stimulus, while being otherwise unconstrained. This allows researchers to visualize or sonify the invariances learned by the model at different processing stages. Unlike traditional human perceptual metamers, which are indistinguishable to humans, model metamers are designed to be indistinguishable to a specific model stage.
    
- **Behavioral Assay:** Human recognition judgments were used to assess whether model metamers reflected human-like invariances. If a model's invariances align with human perception, then humans should recognize the model metamer as belonging to the same category as the original natural stimulus.
    
- **Comparison Across Model Stages:** Metamers were generated from different stages (early to late) of various DNN architectures to identify where in the processing hierarchy human-model discrepancies arise.
    
- **Optimization Procedure:** Metamers were generated by iteratively modifying a white noise signal via gradient descent to minimize the difference between its activations and those of a natural stimulus at a target model stage. Success was confirmed by high correlation between activations and, for classification models, matching classification decisions.
    

### Core Findings

1. **Divergent Invariances in Standard DNNs:** For both visual (ImageNet1K-trained models like ResNet50, CORnet-S, VGG-19, ResNet101, AlexNet) and auditory (speech recognition models like CochCNN9, CochResNet50) DNNs, metamers generated from late model stages were consistently unrecognizable or misclassified by human observers. This indicates that many invariances learned by these models are not present in human sensory systems.
    
2. **Discrepancy Across Training Paradigms:** The divergence in invariances was not exclusive to supervised learning. Self-supervised models (SimCLR, MoCo V2, BYOL, IPCL) also produced metamers that were unrecognizable to humans at late stages, suggesting this is a general failure mode of current DNNs, not just an artifact of explicit category labeling.
    
3. **Texture Bias Not the Sole Explanation:** Even models trained with Stylized ImageNet, which reduces their reliance on texture cues and increases shape bias (making them more human-like in this regard), did not yield significantly more recognizable metamers.
    
4. **Adversarial Training Improves Recognizability:** Adversarial training, a technique to make models more robust to small, imperceptible input perturbations, surprisingly led to more human-recognizable metamers. This suggests that adversarial robustness and human-like invariances might be linked.
    
5. **Metamers as a Complementary Benchmark:** Metamer recognizability dissociated from traditional brain-based benchmarks (like neural prediction metrics and representational similarity analysis with fMRI data) and adversarial vulnerability. This positions the metamer test as a distinct and valuable tool for evaluating and improving sensory models, revealing a different type of model discrepancy.
    
6. **Idiosyncratic Model Invariances:** The human recognizability of a model's metamers was well predicted by other models' recognition of the same metamers, indicating that models develop idiosyncratic invariances beyond what is required by the task.
    

### Implications for Neuroscience

The findings highlight a fundamental qualitative gap between current computational models of sensory systems and their biological counterparts. While DNNs excel at many tasks and predict neural responses, their underlying representational invariances differ significantly from human perception. The metamer test provides neuroscientists with a powerful, general method to assess these discrepancies, pinpointing where in the model's hierarchy these divergences occur. This method can guide the development of next-generation brain models by revealing specific failure modes not captured by other metrics and demonstrating that divergence is not inevitable, as shown by the improvements with adversarial training.

The study suggests that future models of sensory systems should aim to better align their internal invariances with those of biological systems, potentially through alternative training objectives (e.g., generative models, multi-task learning) or architectural modifications that encourage human-like invariant representations.