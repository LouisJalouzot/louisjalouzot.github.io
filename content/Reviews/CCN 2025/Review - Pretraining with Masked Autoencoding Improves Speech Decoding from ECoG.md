---
createdDate: 2025-03-16
publish: true
tags:
  - ECoG
  - Transformers
  - Pretraining
project:
  - Decoding
type:
  - review
originalReviewer: Alexis Thual
cycle: CCN2025
deadline: 2025-03-31
reviewStatus: to do
---
[[Pretraining with Masked Autoencoding Improves Speech Decoding from ECoG.pdf]]

# Interest
Specialized (primarily relevant to experts in a specific subfield)
# Soundness
Adequate (appropriate methodology; evidence consistent with claims)
# Expertise
Fair (I have a good understanding of this work)
# Clarity
Adequate (understandable to an expert audience)

# Comments
- Thanks to pretraining with masked autoencoding and data augmentation the authors successfully improved single word decoding performance for 3 of the 4 subjects in their ECoG dataset
- They used a transformer architecture and beat an RNN baseline from previous work on the same dataset
- 
- Line numbers are missing from the PDF